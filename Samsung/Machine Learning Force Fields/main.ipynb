{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:29:51.344058Z",
     "start_time": "2024-08-02T15:29:46.460438Z"
    }
   },
   "id": "fc59d43911ca3973",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def parse_xyz(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(lines):\n",
    "        num_atoms = int(lines[start_idx].strip())\n",
    "        properties_line = lines[start_idx + 1].strip()\n",
    "        \n",
    "        energy = float(properties_line.split('energy=')[1].split()[0])\n",
    "        \n",
    "        atom_data = []\n",
    "        for i in range(start_idx + 2, start_idx + 2 + num_atoms):\n",
    "            parts = lines[i].split()\n",
    "            atom_type = parts[0]\n",
    "            position = [float(parts[1]), float(parts[2]), float(parts[3])]\n",
    "            force = [float(parts[4]), float(parts[5]), float(parts[6])]\n",
    "            atom_data.append((atom_type, position, force))\n",
    "        \n",
    "        data.append((energy, atom_data))\n",
    "        start_idx += 2 + num_atoms\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_dataframe(data):\n",
    "    rows = []\n",
    "    for block_id, (energy, atom_data) in enumerate(data):\n",
    "        positions_x = []\n",
    "        positions_y = []\n",
    "        positions_z = []\n",
    "        atom_types = []\n",
    "        forces_x = []\n",
    "        forces_y = []\n",
    "        forces_z = []\n",
    "        for atom in atom_data:\n",
    "            atom_type, position, force = atom\n",
    "            atom_types.append(atom_type)\n",
    "            positions_x.append(position[0])\n",
    "            positions_y.append(position[1])\n",
    "            positions_z.append(position[2])\n",
    "            forces_x.append(force[0])\n",
    "            forces_y.append(force[1])\n",
    "            forces_z.append(force[2])\n",
    "        \n",
    "        row = {\n",
    "            'block_id': block_id,  # 블록 ID 추가\n",
    "            'positions_x': positions_x,\n",
    "            'positions_y': positions_y,\n",
    "            'positions_z': positions_z,\n",
    "            'atom_types': atom_types,\n",
    "            'forces_x': forces_x,\n",
    "            'forces_y': forces_y,\n",
    "            'forces_z': forces_z,\n",
    "            'energy': energy,\n",
    "        }\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def encode_atom_types(atom_types):\n",
    "    encoded = []\n",
    "    for atom in atom_types:\n",
    "        encoded.append(atom_type_to_index[atom])\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# 특징 벡터 생성\n",
    "def create_feature_vector(df):\n",
    "    X_positions_x = np.array([np.array(pos) for pos in df['positions_x']])\n",
    "    X_positions_y = np.array([np.array(pos) for pos in df['positions_y']])\n",
    "    X_positions_z = np.array([np.array(pos) for pos in df['positions_z']])\n",
    "    X_atom_types = np.array([np.array(atom_types) for atom_types in df['encoded_atom_types']])\n",
    "    y_energy = df['energy'].values\n",
    "    y_forces_x = np.array([np.array(forces) for forces in df['forces_x']])\n",
    "    y_forces_y = np.array([np.array(forces) for forces in df['forces_y']])\n",
    "    y_forces_z = np.array([np.array(forces) for forces in df['forces_z']])\n",
    "    return X_positions_x, X_positions_y, X_positions_z, X_atom_types, y_energy, y_forces_x, y_forces_y, y_forces_z\n",
    "\n",
    "\n",
    "\n",
    "class LatticeDataset(Dataset):\n",
    "    def __init__(self, X_positions_x, X_positions_y, X_positions_z, X_atom_types, y_energy, y_forces_x, y_forces_y, y_forces_z):\n",
    "        self.X_positions_x = torch.tensor(X_positions_x, dtype=torch.float32)\n",
    "        self.X_positions_y = torch.tensor(X_positions_y, dtype=torch.float32)\n",
    "        self.X_positions_z = torch.tensor(X_positions_z, dtype=torch.float32)\n",
    "        self.X_atom_types = torch.tensor(X_atom_types, dtype=torch.float32)\n",
    "        self.y_energy = torch.tensor(y_energy, dtype=torch.float32)\n",
    "        self.y_forces_x = torch.tensor(y_forces_x, dtype=torch.float32)\n",
    "        self.y_forces_y = torch.tensor(y_forces_y, dtype=torch.float32)\n",
    "        self.y_forces_z = torch.tensor(y_forces_z, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_positions_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X_positions_x[idx], self.X_positions_y[idx], self.X_positions_z[idx], self.X_atom_types[idx],\n",
    "                self.y_energy[idx], self.y_forces_x[idx], self.y_forces_y[idx], self.y_forces_z[idx])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:29:51.374061Z",
     "start_time": "2024-08-02T15:29:51.346057Z"
    }
   },
   "id": "d60828e7c3075892",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 파일에서 데이터 추출\n",
    "train_data = parse_xyz('data/Train.xyz')\n",
    "test_data = parse_xyz('data/Test.xyz')\n",
    "\n",
    "# 데이터프레임으로 변환 및 병합\n",
    "train_df = create_dataframe(train_data)\n",
    "test_df = create_dataframe(test_data)\n",
    "\n",
    "# 원자 종류를 인코딩\n",
    "unique_atom_types = sorted(set([atom for sublist in train_df['atom_types'] for atom in sublist]))\n",
    "atom_type_to_index = {atom: idx for idx, atom in enumerate(unique_atom_types)}\n",
    "\n",
    "train_df['encoded_atom_types'] = train_df['atom_types'].apply(encode_atom_types)\n",
    "test_df['encoded_atom_types'] = test_df['atom_types'].apply(encode_atom_types)\n",
    "\n",
    "X_train_positions_x, X_train_positions_y, X_train_positions_z, X_train_atom_types, y_train_energy, y_train_forces_x, y_train_forces_y, y_train_forces_z = create_feature_vector(train_df)\n",
    "X_test_positions_x, X_test_positions_y, X_test_positions_z, X_test_atom_types, _, _, _, _ = create_feature_vector(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:29:54.554261Z",
     "start_time": "2024-08-02T15:29:51.376066Z"
    }
   },
   "id": "78b6e59cf768f208",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LatticeEnergyForceModel(nn.Module):\n",
    "    def __init__(self, input_dim_positions, input_dim_atom_types, num_atoms):\n",
    "        super(LatticeEnergyForceModel, self).__init__()\n",
    "        self.fc1_x = nn.Linear(input_dim_positions, 128)\n",
    "        self.fc1_y = nn.Linear(input_dim_positions, 128)\n",
    "        self.fc1_z = nn.Linear(input_dim_positions, 128)\n",
    "        self.fc2_x = nn.Linear(128, 64)\n",
    "        self.fc2_y = nn.Linear(128, 64)\n",
    "        self.fc2_z = nn.Linear(128, 64)\n",
    "        self.force_output_x = nn.Linear(64, num_atoms)\n",
    "        self.force_output_y = nn.Linear(64, num_atoms)\n",
    "        self.force_output_z = nn.Linear(64, num_atoms)\n",
    "        self.fc3_energy_x = nn.Linear(64, 32)\n",
    "        self.fc3_energy_y = nn.Linear(64, 32)\n",
    "        self.fc3_energy_z = nn.Linear(64, 32)\n",
    "        self.fc3_energy_atom = nn.Linear(input_dim_atom_types, 32)\n",
    "        self.energy_output = nn.Linear(64, 1)  # 32 (mean of x, y, z) + 32 (atom types)\n",
    "    \n",
    "    def forward(self, positions_x, positions_y, positions_z, atom_types):\n",
    "        x_x = torch.relu(self.fc1_x(positions_x))\n",
    "        x_x = torch.relu(self.fc2_x(x_x))\n",
    "        forces_x = self.force_output_x(x_x)\n",
    "        \n",
    "        x_y = torch.relu(self.fc1_y(positions_y))\n",
    "        x_y = torch.relu(self.fc2_y(x_y))\n",
    "        forces_y = self.force_output_y(x_y)\n",
    "        \n",
    "        x_z = torch.relu(self.fc1_z(positions_z))\n",
    "        x_z = torch.relu(self.fc2_z(x_z))\n",
    "        forces_z = self.force_output_z(x_z)\n",
    "        \n",
    "        energy_x = torch.relu(self.fc3_energy_x(x_x))\n",
    "        energy_y = torch.relu(self.fc3_energy_y(x_y))\n",
    "        energy_z = torch.relu(self.fc3_energy_z(x_z))\n",
    "        \n",
    "        energy_xyz_mean = (energy_x + energy_y + energy_z) / 3\n",
    "        energy_atom = torch.relu(self.fc3_energy_atom(atom_types))\n",
    "        \n",
    "        energy_concat = torch.cat((energy_xyz_mean, energy_atom), dim=1)\n",
    "        energy = self.energy_output(energy_concat)\n",
    "        \n",
    "        forces = torch.stack((forces_x, forces_y, forces_z), dim=-1)\n",
    "        return energy, forces"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:29:54.569645Z",
     "start_time": "2024-08-02T15:29:54.556261Z"
    }
   },
   "id": "cce70d17e76b0510",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# CUDA 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler_positions_x = StandardScaler()\n",
    "scaler_positions_y = StandardScaler()\n",
    "scaler_positions_z = StandardScaler()\n",
    "scaler_atom_types = StandardScaler()\n",
    "\n",
    "X_train_positions_x_scaled = scaler_positions_x.fit_transform(X_train_positions_x)\n",
    "X_train_positions_y_scaled = scaler_positions_y.fit_transform(X_train_positions_y)\n",
    "X_train_positions_z_scaled = scaler_positions_z.fit_transform(X_train_positions_z)\n",
    "X_train_atom_types_scaled = scaler_atom_types.fit_transform(X_train_atom_types)\n",
    "\n",
    "X_test_positions_x_scaled = scaler_positions_x.transform(X_test_positions_x)\n",
    "X_test_positions_y_scaled = scaler_positions_y.transform(X_test_positions_y)\n",
    "X_test_positions_z_scaled = scaler_positions_z.transform(X_test_positions_z)\n",
    "X_test_atom_types_scaled = scaler_atom_types.transform(X_test_atom_types)\n",
    "\n",
    "train_dataset = LatticeDataset(X_train_positions_x_scaled, X_train_positions_y_scaled, X_train_positions_z_scaled, X_train_atom_types_scaled, y_train_energy, y_train_forces_x, y_train_forces_y, y_train_forces_z)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = LatticeDataset(X_test_positions_x_scaled, X_test_positions_y_scaled, X_test_positions_z_scaled, X_test_atom_types_scaled, np.zeros(len(X_test_atom_types_scaled)), np.zeros(len(X_test_atom_types_scaled)), np.zeros(len(X_test_atom_types_scaled)), np.zeros(len(X_test_atom_types_scaled)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_dim_positions = X_train_positions_x_scaled.shape[1]\n",
    "input_dim_atom_types = X_train_atom_types_scaled.shape[1]\n",
    "num_atoms = X_train_positions_x_scaled.shape[1]\n",
    "model = LatticeEnergyForceModel(input_dim_positions, input_dim_atom_types, num_atoms).to(device)\n",
    "criterion_energy = nn.MSELoss()\n",
    "criterion_forces = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:29:55.881291Z",
     "start_time": "2024-08-02T15:29:54.573646Z"
    }
   },
   "id": "d336246ed6aaa7c0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 27.9131\n",
      "Epoch 2/1000, Loss: 27.1047\n",
      "Epoch 3/1000, Loss: 27.7989\n",
      "Epoch 4/1000, Loss: 27.1324\n",
      "Epoch 5/1000, Loss: 29.7689\n",
      "Epoch 6/1000, Loss: 43.8973\n",
      "Epoch 7/1000, Loss: 37.3070\n",
      "Epoch 8/1000, Loss: 26.3413\n",
      "Epoch 9/1000, Loss: 21.4528\n",
      "Epoch 10/1000, Loss: 20.4987\n",
      "Epoch 11/1000, Loss: 18.9448\n",
      "Epoch 12/1000, Loss: 20.3234\n",
      "Epoch 13/1000, Loss: 21.1597\n",
      "Epoch 14/1000, Loss: 17.3260\n",
      "Epoch 15/1000, Loss: 17.3108\n",
      "Epoch 16/1000, Loss: 17.5825\n",
      "Epoch 17/1000, Loss: 14.8268\n",
      "Epoch 18/1000, Loss: 20.3929\n",
      "Epoch 19/1000, Loss: 23.6551\n",
      "Epoch 20/1000, Loss: 24.5471\n",
      "Epoch 21/1000, Loss: 33.6382\n",
      "Epoch 22/1000, Loss: 51.0813\n",
      "Epoch 23/1000, Loss: 68.4979\n",
      "Epoch 24/1000, Loss: 42.4240\n",
      "Epoch 25/1000, Loss: 36.7160\n",
      "Epoch 26/1000, Loss: 31.1043\n",
      "Epoch 27/1000, Loss: 23.0525\n",
      "Epoch 28/1000, Loss: 24.2314\n",
      "Epoch 29/1000, Loss: 18.6541\n",
      "Epoch 30/1000, Loss: 18.2283\n",
      "Epoch 31/1000, Loss: 20.1123\n",
      "Epoch 32/1000, Loss: 18.5343\n",
      "Epoch 33/1000, Loss: 15.6439\n",
      "Epoch 34/1000, Loss: 14.3549\n",
      "Epoch 35/1000, Loss: 15.0999\n",
      "Epoch 36/1000, Loss: 15.8008\n",
      "Epoch 37/1000, Loss: 17.1133\n",
      "Epoch 38/1000, Loss: 19.3351\n",
      "Epoch 39/1000, Loss: 21.0140\n",
      "Epoch 40/1000, Loss: 28.6110\n",
      "Epoch 41/1000, Loss: 31.7872\n",
      "Epoch 42/1000, Loss: 30.0374\n",
      "Epoch 43/1000, Loss: 31.1158\n",
      "Epoch 44/1000, Loss: 38.5739\n",
      "Epoch 45/1000, Loss: 69.3939\n",
      "Epoch 46/1000, Loss: 51.5223\n",
      "Epoch 47/1000, Loss: 45.5674\n",
      "Epoch 48/1000, Loss: 39.4816\n",
      "Epoch 49/1000, Loss: 31.9089\n",
      "Epoch 50/1000, Loss: 22.5632\n",
      "Epoch 51/1000, Loss: 21.2996\n",
      "Epoch 52/1000, Loss: 19.7224\n",
      "Epoch 53/1000, Loss: 20.8803\n",
      "Epoch 54/1000, Loss: 19.1564\n",
      "Epoch 55/1000, Loss: 16.2346\n",
      "Epoch 56/1000, Loss: 16.4949\n",
      "Epoch 57/1000, Loss: 16.2138\n",
      "Epoch 58/1000, Loss: 15.8805\n",
      "Epoch 59/1000, Loss: 17.0570\n",
      "Epoch 60/1000, Loss: 17.5855\n",
      "Epoch 61/1000, Loss: 15.7504\n",
      "Epoch 62/1000, Loss: 16.3847\n",
      "Epoch 63/1000, Loss: 19.3149\n",
      "Epoch 64/1000, Loss: 21.9289\n",
      "Epoch 65/1000, Loss: 23.0200\n",
      "Epoch 66/1000, Loss: 31.8026\n",
      "Epoch 67/1000, Loss: 26.3520\n",
      "Epoch 68/1000, Loss: 34.8884\n",
      "Epoch 69/1000, Loss: 62.1565\n",
      "Epoch 70/1000, Loss: 70.2450\n",
      "Epoch 71/1000, Loss: 39.1388\n",
      "Epoch 72/1000, Loss: 29.7353\n",
      "Epoch 73/1000, Loss: 23.1293\n",
      "Epoch 74/1000, Loss: 18.5275\n",
      "Epoch 75/1000, Loss: 16.5466\n",
      "Epoch 76/1000, Loss: 17.4348\n",
      "Epoch 77/1000, Loss: 19.7678\n",
      "Epoch 78/1000, Loss: 17.6488\n",
      "Epoch 79/1000, Loss: 17.0597\n",
      "Epoch 80/1000, Loss: 16.9260\n",
      "Epoch 81/1000, Loss: 14.1767\n",
      "Epoch 82/1000, Loss: 17.2612\n",
      "Epoch 83/1000, Loss: 15.6442\n",
      "Epoch 84/1000, Loss: 15.8625\n",
      "Epoch 85/1000, Loss: 19.2339\n",
      "Epoch 86/1000, Loss: 20.1206\n",
      "Epoch 87/1000, Loss: 19.9891\n",
      "Epoch 88/1000, Loss: 21.5857\n",
      "Epoch 89/1000, Loss: 25.8569\n",
      "Epoch 90/1000, Loss: 27.3598\n",
      "Epoch 91/1000, Loss: 35.1840\n",
      "Epoch 92/1000, Loss: 50.6954\n",
      "Epoch 93/1000, Loss: 47.0641\n",
      "Epoch 94/1000, Loss: 36.6895\n",
      "Epoch 95/1000, Loss: 35.0638\n",
      "Epoch 96/1000, Loss: 30.2105\n",
      "Epoch 97/1000, Loss: 30.9088\n",
      "Epoch 98/1000, Loss: 22.9791\n",
      "Epoch 99/1000, Loss: 24.7876\n",
      "Epoch 100/1000, Loss: 26.9585\n",
      "Epoch 101/1000, Loss: 29.8353\n",
      "Epoch 102/1000, Loss: 23.9019\n",
      "Epoch 103/1000, Loss: 21.0006\n",
      "Epoch 104/1000, Loss: 25.6764\n",
      "Epoch 105/1000, Loss: 20.8906\n",
      "Epoch 106/1000, Loss: 19.4632\n",
      "Epoch 107/1000, Loss: 17.6556\n",
      "Epoch 108/1000, Loss: 17.3980\n",
      "Epoch 109/1000, Loss: 16.2958\n",
      "Epoch 110/1000, Loss: 16.5928\n",
      "Epoch 111/1000, Loss: 18.0318\n",
      "Epoch 112/1000, Loss: 16.9849\n",
      "Epoch 113/1000, Loss: 23.5402\n",
      "Epoch 114/1000, Loss: 24.5503\n",
      "Epoch 115/1000, Loss: 31.7098\n",
      "Epoch 116/1000, Loss: 40.1024\n",
      "Epoch 117/1000, Loss: 41.3216\n",
      "Epoch 118/1000, Loss: 35.9690\n",
      "Epoch 119/1000, Loss: 35.7059\n",
      "Epoch 120/1000, Loss: 30.9410\n",
      "Epoch 121/1000, Loss: 24.7483\n",
      "Epoch 122/1000, Loss: 21.4894\n",
      "Epoch 123/1000, Loss: 19.5784\n",
      "Epoch 124/1000, Loss: 18.3137\n",
      "Epoch 125/1000, Loss: 16.5207\n",
      "Epoch 126/1000, Loss: 18.3933\n",
      "Epoch 127/1000, Loss: 17.5847\n",
      "Epoch 128/1000, Loss: 29.3762\n",
      "Epoch 129/1000, Loss: 29.5831\n",
      "Epoch 130/1000, Loss: 28.9027\n",
      "Epoch 131/1000, Loss: 25.6307\n",
      "Epoch 132/1000, Loss: 31.7443\n",
      "Epoch 133/1000, Loss: 35.0606\n",
      "Epoch 134/1000, Loss: 36.5875\n",
      "Epoch 135/1000, Loss: 31.0898\n",
      "Epoch 136/1000, Loss: 34.7696\n",
      "Epoch 137/1000, Loss: 41.3913\n",
      "Epoch 138/1000, Loss: 41.1925\n",
      "Epoch 139/1000, Loss: 42.1261\n",
      "Epoch 140/1000, Loss: 29.7465\n",
      "Epoch 141/1000, Loss: 21.1628\n",
      "Epoch 142/1000, Loss: 17.9293\n",
      "Epoch 143/1000, Loss: 17.2361\n",
      "Epoch 144/1000, Loss: 15.7799\n",
      "Epoch 145/1000, Loss: 14.2143\n",
      "Epoch 146/1000, Loss: 14.3956\n",
      "Epoch 147/1000, Loss: 14.2466\n",
      "Epoch 148/1000, Loss: 14.2977\n",
      "Epoch 149/1000, Loss: 17.3017\n",
      "Epoch 150/1000, Loss: 15.5096\n",
      "Epoch 151/1000, Loss: 16.1551\n",
      "Epoch 152/1000, Loss: 16.8260\n",
      "Epoch 153/1000, Loss: 17.7447\n",
      "Epoch 154/1000, Loss: 19.1126\n",
      "Epoch 155/1000, Loss: 20.4540\n",
      "Epoch 156/1000, Loss: 18.8237\n",
      "Epoch 157/1000, Loss: 22.3517\n",
      "Epoch 158/1000, Loss: 52.3981\n",
      "Epoch 159/1000, Loss: 40.2073\n",
      "Epoch 160/1000, Loss: 33.9333\n",
      "Epoch 161/1000, Loss: 41.2046\n",
      "Epoch 162/1000, Loss: 49.7478\n",
      "Epoch 163/1000, Loss: 41.2991\n",
      "Epoch 164/1000, Loss: 35.0659\n",
      "Epoch 165/1000, Loss: 29.6754\n",
      "Epoch 166/1000, Loss: 22.9348\n",
      "Epoch 167/1000, Loss: 18.0905\n",
      "Epoch 168/1000, Loss: 16.3285\n",
      "Epoch 169/1000, Loss: 15.6183\n",
      "Epoch 170/1000, Loss: 14.5817\n",
      "Epoch 171/1000, Loss: 17.7601\n",
      "Epoch 172/1000, Loss: 17.3852\n",
      "Epoch 173/1000, Loss: 18.8837\n",
      "Epoch 174/1000, Loss: 18.1857\n",
      "Epoch 175/1000, Loss: 20.8340\n",
      "Epoch 176/1000, Loss: 20.8688\n",
      "Epoch 177/1000, Loss: 27.3139\n",
      "Epoch 178/1000, Loss: 30.8286\n",
      "Epoch 179/1000, Loss: 33.4647\n",
      "Epoch 180/1000, Loss: 30.1016\n",
      "Epoch 181/1000, Loss: 32.3547\n",
      "Epoch 182/1000, Loss: 24.8727\n",
      "Epoch 183/1000, Loss: 23.8073\n",
      "Epoch 184/1000, Loss: 21.3000\n",
      "Epoch 185/1000, Loss: 22.1820\n",
      "Epoch 186/1000, Loss: 24.3283\n",
      "Epoch 187/1000, Loss: 20.4612\n",
      "Epoch 188/1000, Loss: 22.2718\n",
      "Epoch 189/1000, Loss: 20.2368\n",
      "Epoch 190/1000, Loss: 17.2767\n",
      "Epoch 191/1000, Loss: 19.2988\n",
      "Epoch 192/1000, Loss: 60.2731\n",
      "Epoch 193/1000, Loss: 61.0379\n",
      "Epoch 194/1000, Loss: 59.5851\n",
      "Epoch 195/1000, Loss: 36.6183\n",
      "Epoch 196/1000, Loss: 24.0765\n",
      "Epoch 197/1000, Loss: 20.8286\n",
      "Epoch 198/1000, Loss: 23.6939\n",
      "Epoch 199/1000, Loss: 24.6628\n",
      "Epoch 200/1000, Loss: 22.5039\n",
      "Epoch 201/1000, Loss: 17.0640\n",
      "Epoch 202/1000, Loss: 15.8771\n",
      "Epoch 203/1000, Loss: 16.4990\n",
      "Epoch 204/1000, Loss: 15.4236\n",
      "Epoch 205/1000, Loss: 14.6548\n",
      "Epoch 206/1000, Loss: 13.8603\n",
      "Epoch 207/1000, Loss: 13.9585\n",
      "Epoch 208/1000, Loss: 14.8335\n",
      "Epoch 209/1000, Loss: 16.3205\n",
      "Epoch 210/1000, Loss: 14.9072\n",
      "Epoch 211/1000, Loss: 14.6526\n",
      "Epoch 212/1000, Loss: 16.5097\n",
      "Epoch 213/1000, Loss: 16.9151\n",
      "Epoch 214/1000, Loss: 18.3544\n",
      "Epoch 215/1000, Loss: 19.4256\n",
      "Epoch 216/1000, Loss: 20.3181\n",
      "Epoch 217/1000, Loss: 21.0496\n",
      "Epoch 218/1000, Loss: 26.7564\n",
      "Epoch 219/1000, Loss: 41.2060\n",
      "Epoch 220/1000, Loss: 41.4162\n",
      "Epoch 221/1000, Loss: 42.4585\n",
      "Epoch 222/1000, Loss: 48.6594\n",
      "Epoch 223/1000, Loss: 33.5393\n",
      "Epoch 224/1000, Loss: 36.9812\n",
      "Epoch 225/1000, Loss: 34.0853\n",
      "Epoch 226/1000, Loss: 33.4901\n",
      "Epoch 227/1000, Loss: 26.6638\n",
      "Epoch 228/1000, Loss: 24.5173\n",
      "Epoch 229/1000, Loss: 23.7290\n",
      "Epoch 230/1000, Loss: 24.0940\n",
      "Epoch 231/1000, Loss: 24.3151\n",
      "Epoch 232/1000, Loss: 29.0021\n",
      "Epoch 233/1000, Loss: 19.5329\n",
      "Epoch 234/1000, Loss: 18.6587\n",
      "Epoch 235/1000, Loss: 17.7650\n",
      "Epoch 236/1000, Loss: 19.7075\n",
      "Epoch 237/1000, Loss: 22.5326\n",
      "Epoch 238/1000, Loss: 18.8246\n",
      "Epoch 239/1000, Loss: 18.8352\n",
      "Epoch 240/1000, Loss: 17.0281\n",
      "Epoch 241/1000, Loss: 16.4355\n",
      "Epoch 242/1000, Loss: 17.2087\n",
      "Epoch 243/1000, Loss: 19.1976\n",
      "Epoch 244/1000, Loss: 20.6063\n",
      "Epoch 245/1000, Loss: 24.3258\n",
      "Epoch 246/1000, Loss: 21.9592\n",
      "Epoch 247/1000, Loss: 22.6802\n",
      "Epoch 248/1000, Loss: 21.2941\n",
      "Epoch 249/1000, Loss: 23.1450\n",
      "Epoch 250/1000, Loss: 42.6025\n",
      "Epoch 251/1000, Loss: 40.2578\n",
      "Epoch 252/1000, Loss: 30.7536\n",
      "Epoch 253/1000, Loss: 35.0732\n",
      "Epoch 254/1000, Loss: 29.1263\n",
      "Epoch 255/1000, Loss: 28.2799\n",
      "Epoch 256/1000, Loss: 22.7786\n",
      "Epoch 257/1000, Loss: 24.1552\n",
      "Epoch 258/1000, Loss: 24.0110\n",
      "Epoch 259/1000, Loss: 20.2964\n",
      "Epoch 260/1000, Loss: 19.8863\n",
      "Epoch 261/1000, Loss: 18.3358\n",
      "Epoch 262/1000, Loss: 20.5815\n",
      "Epoch 263/1000, Loss: 23.7588\n",
      "Epoch 264/1000, Loss: 18.5570\n",
      "Epoch 265/1000, Loss: 18.2744\n",
      "Epoch 266/1000, Loss: 23.5105\n",
      "Epoch 267/1000, Loss: 31.5269\n",
      "Epoch 268/1000, Loss: 23.4097\n",
      "Epoch 269/1000, Loss: 19.1235\n",
      "Epoch 270/1000, Loss: 19.7396\n",
      "Epoch 271/1000, Loss: 17.4677\n",
      "Epoch 272/1000, Loss: 16.9785\n",
      "Epoch 273/1000, Loss: 22.1152\n",
      "Epoch 274/1000, Loss: 37.3165\n",
      "Epoch 275/1000, Loss: 47.4027\n",
      "Epoch 276/1000, Loss: 37.2168\n",
      "Epoch 277/1000, Loss: 30.1078\n",
      "Epoch 278/1000, Loss: 24.2162\n",
      "Epoch 279/1000, Loss: 24.2063\n",
      "Epoch 280/1000, Loss: 21.9073\n",
      "Epoch 281/1000, Loss: 18.9260\n",
      "Epoch 282/1000, Loss: 17.4018\n",
      "Epoch 283/1000, Loss: 17.9284\n",
      "Epoch 284/1000, Loss: 19.2171\n",
      "Epoch 285/1000, Loss: 19.6323\n",
      "Epoch 286/1000, Loss: 18.0029\n",
      "Epoch 287/1000, Loss: 17.2614\n",
      "Epoch 288/1000, Loss: 17.8757\n",
      "Epoch 289/1000, Loss: 17.7169\n",
      "Epoch 290/1000, Loss: 17.1075\n",
      "Epoch 291/1000, Loss: 22.1129\n",
      "Epoch 292/1000, Loss: 42.5429\n",
      "Epoch 293/1000, Loss: 39.7551\n",
      "Epoch 294/1000, Loss: 64.5552\n",
      "Epoch 295/1000, Loss: 41.2763\n",
      "Epoch 296/1000, Loss: 35.2817\n",
      "Epoch 297/1000, Loss: 26.4399\n",
      "Epoch 298/1000, Loss: 22.2161\n",
      "Epoch 299/1000, Loss: 22.9663\n",
      "Epoch 300/1000, Loss: 17.4411\n",
      "Epoch 301/1000, Loss: 14.7531\n",
      "Epoch 302/1000, Loss: 14.4036\n",
      "Epoch 303/1000, Loss: 20.9374\n",
      "Epoch 304/1000, Loss: 18.4884\n",
      "Epoch 305/1000, Loss: 16.0612\n",
      "Epoch 306/1000, Loss: 21.7133\n",
      "Epoch 307/1000, Loss: 26.2430\n",
      "Epoch 308/1000, Loss: 24.1231\n",
      "Epoch 309/1000, Loss: 20.1638\n",
      "Epoch 310/1000, Loss: 20.3525\n",
      "Epoch 311/1000, Loss: 20.8579\n",
      "Epoch 312/1000, Loss: 23.8850\n",
      "Epoch 313/1000, Loss: 26.7534\n",
      "Epoch 314/1000, Loss: 25.3036\n",
      "Epoch 315/1000, Loss: 25.7352\n",
      "Epoch 316/1000, Loss: 26.2285\n",
      "Epoch 317/1000, Loss: 25.5899\n",
      "Epoch 318/1000, Loss: 22.0890\n",
      "Epoch 319/1000, Loss: 19.9206\n",
      "Epoch 320/1000, Loss: 19.5950\n",
      "Epoch 321/1000, Loss: 21.0022\n",
      "Epoch 322/1000, Loss: 30.5256\n",
      "Epoch 323/1000, Loss: 27.0082\n",
      "Epoch 324/1000, Loss: 25.2087\n",
      "Epoch 325/1000, Loss: 27.0257\n",
      "Epoch 326/1000, Loss: 22.7988\n",
      "Epoch 327/1000, Loss: 30.5543\n",
      "Epoch 328/1000, Loss: 26.0833\n",
      "Epoch 329/1000, Loss: 24.5338\n",
      "Epoch 330/1000, Loss: 19.8618\n",
      "Epoch 331/1000, Loss: 21.5645\n",
      "Epoch 332/1000, Loss: 17.3836\n",
      "Epoch 333/1000, Loss: 18.6826\n",
      "Epoch 334/1000, Loss: 32.2739\n",
      "Epoch 335/1000, Loss: 33.9091\n",
      "Epoch 336/1000, Loss: 27.8372\n",
      "Epoch 337/1000, Loss: 31.1175\n",
      "Epoch 338/1000, Loss: 26.1954\n",
      "Epoch 339/1000, Loss: 30.3094\n",
      "Epoch 340/1000, Loss: 30.0828\n",
      "Epoch 341/1000, Loss: 36.0486\n",
      "Epoch 342/1000, Loss: 36.2123\n",
      "Epoch 343/1000, Loss: 25.1759\n",
      "Epoch 344/1000, Loss: 19.0256\n",
      "Epoch 345/1000, Loss: 21.9885\n",
      "Epoch 346/1000, Loss: 20.3181\n",
      "Epoch 347/1000, Loss: 18.2015\n",
      "Epoch 348/1000, Loss: 15.3555\n",
      "Epoch 349/1000, Loss: 14.7363\n",
      "Epoch 350/1000, Loss: 14.3421\n",
      "Epoch 351/1000, Loss: 13.6281\n",
      "Epoch 352/1000, Loss: 13.8973\n",
      "Epoch 353/1000, Loss: 14.5080\n",
      "Epoch 354/1000, Loss: 15.8160\n",
      "Epoch 355/1000, Loss: 17.4429\n",
      "Epoch 356/1000, Loss: 29.6181\n",
      "Epoch 357/1000, Loss: 42.8512\n",
      "Epoch 358/1000, Loss: 42.8938\n",
      "Epoch 359/1000, Loss: 33.0014\n",
      "Epoch 360/1000, Loss: 30.6186\n",
      "Epoch 361/1000, Loss: 28.1567\n",
      "Epoch 362/1000, Loss: 26.2131\n",
      "Epoch 363/1000, Loss: 24.4380\n",
      "Epoch 364/1000, Loss: 18.7365\n",
      "Epoch 365/1000, Loss: 16.1549\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m X_batch_positions_x, X_batch_positions_y, X_batch_positions_z, X_batch_atom_types, y_energy_batch, y_forces_x_batch, y_forces_y_batch, y_forces_z_batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m----> 6\u001B[0m     X_batch_positions_x, X_batch_positions_y, X_batch_positions_z, X_batch_atom_types \u001B[38;5;241m=\u001B[39m (X_batch_positions_x\u001B[38;5;241m.\u001B[39mto(device), \u001B[43mX_batch_positions_y\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, X_batch_positions_z\u001B[38;5;241m.\u001B[39mto(device), X_batch_atom_types\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[0;32m      7\u001B[0m     y_energy_batch, y_forces_x_batch, y_forces_y_batch, y_forces_z_batch \u001B[38;5;241m=\u001B[39m (y_energy_batch\u001B[38;5;241m.\u001B[39mto(device), y_forces_x_batch\u001B[38;5;241m.\u001B[39mto(device), y_forces_y_batch\u001B[38;5;241m.\u001B[39mto(device), y_forces_z_batch\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[0;32m      9\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for X_batch_positions_x, X_batch_positions_y, X_batch_positions_z, X_batch_atom_types, y_energy_batch, y_forces_x_batch, y_forces_y_batch, y_forces_z_batch in train_loader:\n",
    "        X_batch_positions_x, X_batch_positions_y, X_batch_positions_z, X_batch_atom_types = (X_batch_positions_x.to(device), X_batch_positions_y.to(device), X_batch_positions_z.to(device), X_batch_atom_types.to(device))\n",
    "        y_energy_batch, y_forces_x_batch, y_forces_y_batch, y_forces_z_batch = (y_energy_batch.to(device), y_forces_x_batch.to(device), y_forces_y_batch.to(device), y_forces_z_batch.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_energy_pred, y_forces_pred = model(X_batch_positions_x, X_batch_positions_y, X_batch_positions_z, X_batch_atom_types)\n",
    "        \n",
    "        energy_loss = criterion_energy(y_energy_pred.squeeze(), y_energy_batch)\n",
    "        forces_loss_x = criterion_forces(y_forces_pred[:, :, 0], y_forces_x_batch)\n",
    "        forces_loss_y = criterion_forces(y_forces_pred[:, :, 1], y_forces_y_batch)\n",
    "        forces_loss_z = criterion_forces(y_forces_pred[:, :, 2], y_forces_z_batch)\n",
    "        forces_loss = forces_loss_x + forces_loss_y + forces_loss_z\n",
    "        \n",
    "        loss = energy_loss + forces_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X_batch_positions_x.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}')\n",
    "\n",
    "# 모델 훈련 후, 훈련 데이터로 검증 수행\n",
    "model.eval()\n",
    "train_energy_loss = 0.0\n",
    "train_forces_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch_positions_x, X_batch_positions_y, X_batch_positions_z, X_batch_atom_types, y_energy_batch, y_forces_x_batch, y_forces_y_batch, y_forces_z_batch in train_loader:\n",
    "        X_batch_positions_x, X_batch_positions_y, X_batch_positions_z, X_batch_atom_types = (X_batch_positions_x.to(device), X_batch_positions_y.to(device), X_batch_positions_z.to(device), X_batch_atom_types.to(device))\n",
    "        y_energy_batch, y_forces_x_batch, y_forces_y_batch, y_forces_z_batch = (y_energy_batch.to(device), y_forces_x_batch.to(device), y_forces_y_batch.to(device), y_forces_z_batch.to(device))\n",
    "        y_energy_pred, y_forces_pred = model(X_batch_positions_x, X_batch_positions_y, X_batch_positions_z, X_batch_atom_types)\n",
    "        \n",
    "        energy_loss = criterion_energy(y_energy_pred.squeeze(), y_energy_batch)\n",
    "        forces_loss_x = criterion_forces(y_forces_pred[:, :, 0], y_forces_x_batch)\n",
    "        forces_loss_y = criterion_forces(y_forces_pred[:, :, 1], y_forces_y_batch)\n",
    "        forces_loss_z = criterion_forces(y_forces_pred[:, :, 2], y_forces_z_batch)\n",
    "        forces_loss = forces_loss_x + forces_loss_y + forces_loss_z\n",
    "        \n",
    "        train_energy_loss += energy_loss.item() * X_batch_positions_x.size(0)\n",
    "        train_forces_loss += forces_loss.item() * X_batch_positions_x.size(0)\n",
    "    \n",
    "    train_energy_loss /= len(train_loader.dataset)\n",
    "    train_forces_loss /= len(train_loader.dataset)\n",
    "    print(f'Validation Energy Loss: {train_energy_loss:.4f}, Validation Forces Loss: {train_forces_loss:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:40:54.714202Z",
     "start_time": "2024-08-02T15:38:05.535072Z"
    }
   },
   "id": "7c960cad861a2431",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict_with_uncertainty(model, positions_x, positions_y, positions_z, atom_types, n_iter=50):\n",
    "    model.train()  # Keep dropout layers in training mode\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        with torch.no_grad():\n",
    "            y_energy_pred, _ = model(positions_x, positions_y, positions_z, atom_types)\n",
    "            predictions.append(y_energy_pred.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    prediction_mean = predictions.mean(axis=0)\n",
    "    prediction_std = predictions.std(axis=0)\n",
    "    \n",
    "    return prediction_mean, prediction_std\n",
    "\n",
    "# 예측 수행 (훈련 및 테스트 데이터)\n",
    "model.eval()\n",
    "X_train_positions_x_tensor = torch.tensor(X_train_positions_x_scaled, dtype=torch.float32).to(device)\n",
    "X_train_positions_y_tensor = torch.tensor(X_train_positions_y_scaled, dtype=torch.float32).to(device)\n",
    "X_train_positions_z_tensor = torch.tensor(X_train_positions_z_scaled, dtype=torch.float32).to(device)\n",
    "X_train_atom_types_tensor = torch.tensor(X_train_atom_types_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "energy_train_mean, energy_train_uncertainty = predict_with_uncertainty(model, X_train_positions_x_tensor, X_train_positions_y_tensor, X_train_positions_z_tensor, X_train_atom_types_tensor, n_iter=50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, y_train_forces_pred = model(X_train_positions_x_tensor, X_train_positions_y_tensor, X_train_positions_z_tensor, X_train_atom_types_tensor)\n",
    "y_train_forces_pred = y_train_forces_pred.cpu().numpy()\n",
    "\n",
    "X_test_positions_x_tensor = torch.tensor(X_test_positions_x_scaled, dtype=torch.float32).to(device)\n",
    "X_test_positions_y_tensor = torch.tensor(X_test_positions_y_scaled, dtype=torch.float32).to(device)\n",
    "X_test_positions_z_tensor = torch.tensor(X_test_positions_z_scaled, dtype=torch.float32).to(device)\n",
    "X_test_atom_types_tensor = torch.tensor(X_test_atom_types_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "energy_test_mean, energy_test_uncertainty = predict_with_uncertainty(model, X_test_positions_x_tensor, X_test_positions_y_tensor, X_test_positions_z_tensor, X_test_atom_types_tensor, n_iter=50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, y_test_forces_pred = model(X_test_positions_x_tensor, X_test_positions_y_tensor, X_test_positions_z_tensor, X_test_atom_types_tensor)\n",
    "y_test_forces_pred = y_test_forces_pred.cpu().numpy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:37:15.306007Z",
     "start_time": "2024-08-02T15:37:14.948917Z"
    }
   },
   "id": "2245e56899384a88",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to train_predictions.csv\n",
      "Results saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 결과 저장\n",
    "def save_results(filename, energy_mean, energy_uncertainty, forces_pred):\n",
    "    results = {\n",
    "        'ID': [f'TEST_{i:04d}' for i in range(len(energy_mean))],\n",
    "        'energy': energy_mean.flatten(),\n",
    "        'energy_uncertainty': energy_uncertainty.flatten(),\n",
    "        'forces': [forces.flatten().tolist() for forces in forces_pred]\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f'Results saved to {filename}')\n",
    "\n",
    "# 훈련 데이터에 대한 결과 저장\n",
    "save_results('train_predictions.csv', energy_train_mean, energy_train_uncertainty, y_train_forces_pred)\n",
    "\n",
    "# 테스트 데이터에 대한 결과 저장\n",
    "save_results('test_predictions.csv', energy_test_mean, energy_test_uncertainty, y_test_forces_pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:37:16.880377Z",
     "start_time": "2024-08-02T15:37:15.308007Z"
    }
   },
   "id": "513bc213b6667ea0",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:37:16.896103Z",
     "start_time": "2024-08-02T15:37:16.881376Z"
    }
   },
   "id": "aca089e96b4c8bc1",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
