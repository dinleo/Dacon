{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFpAi8g9XmUU"
   },
   "source": [
    "# Molecular Dynamics with Allegro\n",
    "\n",
    "### Authors: Simon Batzner and Albert Musaelian, Anders Johansson, Lixin Sun, Boris Kozinsky\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/mir-group/allegro/blob/main/logo.png?raw=true\" width=\"30%\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img src=\"https://github.com/mir-group/nequip/blob/main/logo.png?raw=true\" width=\"30%\">\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6YiV6ShQWB2"
   },
   "source": [
    "## What is this?\n",
    "\n",
    "This is a tutorial for Allegro, an architecture for building highly accurate and scalable Machine Learning Interatomic Potentials (MLIPs). The ideas are described [in this paper](https://arxiv.org/abs/2204.05249). We have released an [open-source package](https://github.com/mir-group/allegro) implementing Allegro, which builds on the `nequip` framework with the goal of making it straightforward to train your own Allegro models with a few simple commands.\n",
    "This tutorial serves as a simple introduction into the Allegro package and `nequip` code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwRtSJt6Y-Ky"
   },
   "source": [
    "## What can it do?\n",
    "\n",
    "The goal of Allegro is to make it as simple as possible to train an acurate, fast, and scalable Machine Learning Interatomic Potential and deploy it in production simulations. You will never have to write a single line of Python, but instead you can train a network with a single command and easily use it to run MD in LAMMPS or ASE. If you need to customize it to your needs, the code is also modular and flexible under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8gImqa_N_e1"
   },
   "source": [
    "## Contents\n",
    "\n",
    "This tutorial will teach you how to:\n",
    "\n",
    "* Train a model\n",
    "* Deploy the model intro production\n",
    "* Run MD with it in LAMMPS\n",
    "* (Optional) Extend the model with custom code\n",
    "\n",
    "Everything will happen in this Colab, including running LAMMPS. Training + inference will take only about 10 minutes. Before you get started, however, you will have to compile LAMMPS which takes **approximately 8 minutes**. Once we have installed NequIP, Allegro, and LAMMPS, we're ready to get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPqnt-SAXyvL"
   },
   "source": [
    "**‚ùó‚ùó‚ùó Confirm device is GPU ‚ùó‚ùó‚ùó**\n",
    "\n",
    "Before you get started, make sure that in your menu bar Runtime ‚Üí Change runtime type is set to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flgydiAt-9mA"
   },
   "source": [
    "## Resources\n",
    "\n",
    "Allegro builds on the NequIP code which you find [here](https://github.com/mir-group/nequip).\n",
    "\n",
    "\n",
    "Papers:\n",
    "\n",
    "[1] NequIP: https://www.nature.com/articles/s41467-022-29939-5\n",
    "\n",
    "[2] Allegro: https://arxiv.org/abs/2204.05249\n",
    "\n",
    "\n",
    "Code:\n",
    "\n",
    "[1] NequIP: https://github.com/mir-group/nequip\n",
    "\n",
    "[2] Allegro: https://github.com/mir-group/allegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR2B0QZWaYKr"
   },
   "source": [
    "## Install\n",
    "\n",
    "This will take approximately 5 minutes since we will be building LAMMPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMv7q0tdPilW",
    "outputId": "2a56c4ff-06cb-4f36-a7c2-31fb2322eceb",
    "ExecuteTime": {
     "end_time": "2024-09-03T12:04:58.432224Z",
     "start_time": "2024-09-03T12:04:31.791575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.15.12)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (5.9.6)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.34.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: pathtools in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (4.8.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nequip==0.5.5\n",
      "  Using cached nequip-0.5.5-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: torch==1.11 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip==0.5.5) (1.26.2)\n",
      "Requirement already satisfied: ase in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip==0.5.5) (3.23.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip==0.5.5) (4.66.1)\n",
      "Requirement already satisfied: e3nn<0.6.0,>=0.3.5 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip==0.5.5) (0.5.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip==0.5.5) (6.0.1)\n",
      "Requirement already satisfied: torch-runstats>=0.2.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip==0.5.5) (0.2.0)\n",
      "Requirement already satisfied: torch-ema>=0.3.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip==0.5.5) (0.3)\n",
      "Requirement already satisfied: scikit-learn<=1.0.1 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip==0.5.5) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==1.11) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.12)\n",
      "Requirement already satisfied: scipy in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.10.1)\n",
      "Requirement already satisfied: opt-einsum-fx>=0.1.4 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (0.1.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<=1.0.1->nequip==0.5.5) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<=1.0.1->nequip==0.5.5) (3.2.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ase->nequip==0.5.5) (3.8.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->nequip==0.5.5) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip==0.5.5) (6.1.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (3.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->e3nn<0.6.0,>=0.3.5->nequip==0.5.5) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.4->ase->nequip==0.5.5) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip==0.5.5) (1.16.0)\n",
      "Using cached nequip-0.5.5-py3-none-any.whl (138 kB)\n",
      "Installing collected packages: nequip\n",
      "  Attempting uninstall: nequip\n",
      "    Found existing installation: nequip 0.6.1\n",
      "    Uninstalling nequip-0.6.1:\n",
      "      Successfully uninstalled nequip-0.6.1\n",
      "Successfully installed nequip-0.5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "fatal: destination path 'allegro' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\dinle\\code\\competition\\dacon\\samsung\\machine learning force fields\\allegro\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nequip>=0.5.3 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mir-allegro==0.2.0) (0.5.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.26.2)\n",
      "Requirement already satisfied: ase in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.23.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.9.0,<=1.12,>=1.8 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.11.0)\n",
      "Requirement already satisfied: e3nn<0.6.0,>=0.3.5 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.1)\n",
      "Requirement already satisfied: torch-runstats>=0.2.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
      "Requirement already satisfied: torch-ema>=0.3.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
      "Requirement already satisfied: scikit-learn<=1.0.1 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from e3nn<0.6.0,>=0.3.5->nequip>=0.5.3->mir-allegro==0.2.0) (1.12)\n",
      "Requirement already satisfied: scipy in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from e3nn<0.6.0,>=0.3.5->nequip>=0.5.3->mir-allegro==0.2.0) (1.10.1)\n",
      "Requirement already satisfied: opt-einsum-fx>=0.1.4 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from e3nn<0.6.0,>=0.3.5->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<=1.0.1->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<=1.0.1->nequip>=0.5.3->mir-allegro==0.2.0) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch!=1.9.0,<=1.12,>=1.8->nequip>=0.5.3->mir-allegro==0.2.0) (4.8.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.8.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->nequip>=0.5.3->mir-allegro==0.2.0) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (6.1.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.3.5->nequip>=0.5.3->mir-allegro==0.2.0) (3.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->e3nn<0.6.0,>=0.3.5->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
      "Building wheels for collected packages: mir-allegro\n",
      "  Building wheel for mir-allegro (setup.py): started\n",
      "  Building wheel for mir-allegro (setup.py): finished with status 'done'\n",
      "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27695 sha256=9e749164ad0e683c0a6437716be97ee21f6f1ef10cbc92d020f1f7e8c1fbe0d8\n",
      "  Stored in directory: C:\\Users\\dinle\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-xuzqit8r\\wheels\\14\\d4\\3c\\fc1db279479e38fcdae0de0b1cd69dd4e8594471e5f5779cca\n",
      "Successfully built mir-allegro\n",
      "Installing collected packages: mir-allegro\n",
      "  Attempting uninstall: mir-allegro\n",
      "    Found existing installation: mir-allegro 0.2.0\n",
      "    Uninstalling mir-allegro-0.2.0:\n",
      "      Successfully uninstalled mir-allegro-0.2.0\n",
      "Successfully installed mir-allegro-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "fatal: destination path 'lammps' already exists and is not an empty directory.\n",
      "fatal: destination path 'pair_allegro' already exists and is not an empty directory.\n",
      "'wget'ÏùÄ(Îäî) ÎÇ¥Î∂Ä ÎòêÎäî Ïô∏Î∂Ä Î™ÖÎ†π, Ïã§ÌñâÌï† Ïàò ÏûàÎäî ÌîÑÎ°úÍ∑∏Îû®, ÎòêÎäî\n",
      "Î∞∞Ïπò ÌååÏùºÏù¥ ÏïÑÎãôÎãàÎã§.\n",
      "patch_lammps.sh: line 3: $'\\r': command not found\n",
      "patch_lammps.sh: line 5: $'\\r': command not found\n",
      "patch_lammps.sh: line 7: syntax error near unexpected token `$'in\\r''\n",
      "patch_lammps.sh: line 7: `   case $option in\n",
      "'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mkl-include in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2024.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "'wget'ÏùÄ(Îäî) ÎÇ¥Î∂Ä ÎòêÎäî Ïô∏Î∂Ä Î™ÖÎ†π, Ïã§ÌñâÌï† Ïàò ÏûàÎäî ÌîÑÎ°úÍ∑∏Îû®, ÎòêÎäî\n",
      "Î∞∞Ïπò ÌååÏùºÏù¥ ÏïÑÎãôÎãàÎã§.\n",
      "'sh'ÏùÄ(Îäî) ÎÇ¥Î∂Ä ÎòêÎäî Ïô∏Î∂Ä Î™ÖÎ†π, Ïã§ÌñâÌï† Ïàò ÏûàÎäî ÌîÑÎ°úÍ∑∏Îû®, ÎòêÎäî\n",
      "Î∞∞Ïπò ÌååÏùºÏù¥ ÏïÑÎãôÎãàÎã§.\n",
      "'rm'ÏùÄ(Îäî) ÎÇ¥Î∂Ä ÎòêÎäî Ïô∏Î∂Ä Î™ÖÎ†π, Ïã§ÌñâÌï† Ïàò ÏûàÎäî ÌîÑÎ°úÍ∑∏Îû®, ÎòêÎäî\n",
      "Î∞∞Ïπò ÌååÏùºÏù¥ ÏïÑÎãôÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "# install wandb\n",
    "!pip install wandb\n",
    "\n",
    "# install nequip\n",
    "!pip install nequip==0.5.5 torch==1.11\n",
    "\n",
    "# fix colab imports\n",
    "import site\n",
    "site.main()\n",
    "\n",
    "# set to allow anonymous WandB\n",
    "import os\n",
    "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
    "\n",
    "# install allegro\n",
    "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
    "!pip install allegro/\n",
    "\n",
    "# clone lammps\n",
    "!git clone --depth=1 https://github.com/lammps/lammps\n",
    "\n",
    "# clone pair_allegro and pair_nequip\n",
    "!git clone --depth 1 https://github.com/mir-group/pair_allegro.git\n",
    "\n",
    "# download libtorch\n",
    "!wget https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.11.0%2Bcu102.zip && unzip -q libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip\n",
    "\n",
    "# patch lammps\n",
    "!cd pair_allegro && bash patch_lammps.sh ../lammps/\n",
    "\n",
    "# install mkl interface\n",
    "!pip install mkl-include\n",
    "\n",
    "# update cmake\n",
    "!wget https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
    "!sh ./cmake-3.23.1-linux-x86_64.sh --prefix=/usr/local --exclude-subdir\n",
    "\n",
    "# build lammps\n",
    "!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=/content/libtorch -DMKL_INCLUDE_DIR=`python -c \"import sysconfig;from pathlib import Path;print(Path(sysconfig.get_paths()[\\\"include\\\"]).parent)\"` && make -j$(nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8-162BFckO5k",
    "ExecuteTime": {
     "end_time": "2024-09-03T12:11:19.114844Z",
     "start_time": "2024-09-03T12:11:14.708503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python imports and pre-definitions\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['font.size'] = 30\n",
    "\n",
    "def parse_lammps_rdf(rdffile):\n",
    "    \"\"\"Parse the RDF file written by LAMMPS\n",
    "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
    "    \"\"\"\n",
    "    with open(rdffile, 'r') as rdfout:\n",
    "        rdfs = []; buffer = []\n",
    "        for line in rdfout:\n",
    "            values = line.split()\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            elif len(values) == 2:\n",
    "                nbins = values[1]\n",
    "            else:\n",
    "                buffer.append([float(values[1]), float(values[2])])\n",
    "                if len(buffer) == int(nbins):\n",
    "                    frame = np.transpose(np.array(buffer))\n",
    "                    rdfs.append(frame)\n",
    "                    buffer = []\n",
    "    return rdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HDmxkn3z8_m"
   },
   "source": [
    "## Workflow\n",
    "\n",
    "Our workflow consists of 3 steps: train, deploy, and run:\n",
    "\n",
    "\n",
    "1. Train: using a data set, train the neural network üß†\n",
    "2. Deploy: convert the Python-based model into a stand-alone potential file for fast execution ‚ö°\n",
    "3. Run: run Molecular Dynamics, Monte Carlo, Structural Minimization, etc. with it in LAMMPS üèÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OD71eeDz7dA"
   },
   "source": [
    "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/all.png?raw=true\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZLeiSDAasHo"
   },
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELdBzH_8z4_2"
   },
   "source": [
    "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/train.png?raw=true\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KuOIippfVfd"
   },
   "source": [
    "Here, we will train an Allegro potential on a high-temperature Si data set sampled with AIMD at $T=800\\mathrm{K}$, consisting of 64 atoms per frame. We will train on only 50 DFT structures. We first download the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "14BpbVG0-qm_",
    "ExecuteTime": {
     "end_time": "2024-08-26T12:33:03.775745Z",
     "start_time": "2024-08-26T12:33:03.656747Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# download data\n",
    "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1FEwF4i8IDHGmAIQ3RilA0jG9_lEX4Yk0?usp=sharing\n",
    "!mkdir Si_data\n",
    "!mv *.xyz ./Si_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1L_tNge-jm1",
    "outputId": "f1b0bed7-9d1f-4f8c-980a-58a009092e4e",
    "ExecuteTime": {
     "end_time": "2024-08-26T12:33:06.192749Z",
     "start_time": "2024-08-26T12:33:06.126746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls'ÏùÄ(Îäî) ÎÇ¥Î∂Ä ÎòêÎäî Ïô∏Î∂Ä Î™ÖÎ†π, Ïã§ÌñâÌï† Ïàò ÏûàÎäî ÌîÑÎ°úÍ∑∏Îû®, ÎòêÎäî\n",
      "Î∞∞Ïπò ÌååÏùºÏù¥ ÏïÑÎãôÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "!ls Si_data"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!unzip si6.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMWEzre27nqj"
   },
   "source": [
    "### Training command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdDPNeFp70w0"
   },
   "source": [
    "We train the Allegro model using the commands from the `nequip` package.\n",
    "The syntax is the following:\n",
    "\n",
    "```\n",
    "nequip-train path/to/config.yaml\n",
    "````\n",
    "We will train here for 100 epochs, this should only take 3 minutes or so. If you get impatient, simply click Runtime ‚Üí Interrupt execution in the menu bar and you will have a less accurate (but probably still accurate enough) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwgd_HMX8vUk"
   },
   "source": [
    "### Logging with Weights and Biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q_GyQfC0npt"
   },
   "source": [
    "Starting a training run with the above command will print output to our console, but it is usually more convenient to view the results in a web interface called [Weights and Biases](wandb.ai). Click the link next to the rocket emoji to watch the run in the WandB interface üöÄ\n",
    "\n",
    "This tutorial is set up to use `wandb` in anonymous mode so that you won't have to sign up for `wandb` if you do not want to. When you use Allegro yourself and want to use `wandb`, you will be presented with a login prompt.\n",
    "\n",
    "In `wandb`, watch the following keys:\n",
    "\n",
    "* Plot 1: `validation_f_mae`, `training_f_mae`\n",
    "* Plot 2: `validation_e/N_mae`, `training_e/N_mae`\n",
    "\n",
    "These are the validation/training error in all force components and the validation/training error in the potential energy, normalized by the number of atoms, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo-CYhLV80MD"
   },
   "source": [
    "### Starting the training\n",
    "\n",
    "We are now ready to train the network:\n",
    "\n",
    "(We add the `--equivariance-test` option to have `nequip-train` automatically run equivariance tests on the model and print the results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oc-i-KbA_2ly",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2b2192a2-78ba-41c1-fca6-3e8eaabd2467",
    "ExecuteTime": {
     "end_time": "2024-09-03T12:13:47.146435Z",
     "start_time": "2024-09-03T12:13:42.608917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\nequip-train.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nequip\\scripts\\train.py\", line 74, in main\n",
      "    trainer = restart(config)\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nequip\\scripts\\train.py\", line 200, in restart\n",
      "    dictionary = load_file(\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nequip\\utils\\savenload.py\", line 274, in load_file\n",
      "    return torch.load(filename)\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\", line 712, in load\n",
      "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\", line 1046, in _load\n",
      "    result = unpickler.load()\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\", line 1016, in persistent_load\n",
      "    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\", line 1001, in load_tensor\n",
      "    wrap_storage=restore_location(storage, location),\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\", line 176, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\", line 152, in _cuda_deserialize\n",
      "    device = validate_cuda_device(location)\n",
      "  File \"C:\\Users\\dinle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\", line 136, in validate_cuda_device\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
     ]
    }
   ],
   "source": [
    "#!rm -rf ./results\n",
    "!nequip-train allegro/configs/tutorial.yaml --equivariance-test"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-03T12:30:20.348131Z",
     "start_time": "2024-09-03T12:30:20.338130Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.1 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from torch==2.3.1) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from torch==2.3.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dinle\\appdata\\roaming\\python\\python39\\site-packages (from torch==2.3.1) (2023.9.2)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1) (2021.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch==2.3.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch==2.3.1) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.3.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-03T12:30:36.615653Z",
     "start_time": "2024-09-03T12:30:21.833346Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.3.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: c:\\users\\dinle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, mkl, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, e3nn, efficientnet-pytorch, nequip, opt-einsum-fx, pretrainedmodels, pytorch-lightning, schnetpack, thop, timm, torch-ema, torchani, torchaudio, torchmetrics, torchvision, ultralytics\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-03T12:30:17.029610Z",
     "start_time": "2024-09-03T12:30:11.004616Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!nequip-evaluate --train-dir results/silicon-tutorial/si --output output.xyz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EwuQZLDO4Cr"
   },
   "source": [
    "We see that the model has converged to an energy accuarcy ~1meV/atom and a force accuracy of ~50 meV/A on the validation set in 3 minutes and trained on only 50 samples. Given the high temperature of the reference data, we are happy with this error and this should give us a good first potential. Note that these numbers will decrease further if you increase the network size, the training set size, and/or the number of epochs to train. We use a simple example here for illustrative purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koEn6QziihbY"
   },
   "source": [
    "### Deciding on hyperparameters\n",
    "\n",
    "The `nequip` framework includes the `nequip-benchmark` tool for quickly assessing the performance of different combinations of model hyperparameters and datasets. When provided with a YAML configuration file exactly like the one provided to `nequip-train`, it constructs a randomly initialized model with the given hyperparameters, loads the specified dataset, and runs a quick benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "227UH5rli49P",
    "outputId": "d9f78d52-f570-42f6-9ec8-24fd94d9aa6c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "Loading dataset... \n",
      "    loading dataset took 0.0132s\n",
      "    loaded dataset of size 110 and sampled --n-data=1 frames\n",
      "    benchmark frames statistics:\n",
      "         number of atoms: 64\n",
      "         number of types: 1\n",
      "          avg. num edges: 1796.0\n",
      "         avg. neigh/atom: 28.0625\n",
      "Building model... \n",
      "    building model took 0.2925s\n",
      "Compile...\n",
      "    compilation took 0.1388s\n",
      "Warmup...\n",
      "    6 calls of warmup took 3.2600s\n",
      "Starting...\n",
      " -- Results --\n",
      "PLEASE NOTE: these are speeds for the MODEL, evaluated on --n-data=1 configurations kept in memory.\n",
      "    \\_ MD itself, memory copies, and other overhead will affect real-world performance.\n",
      "\n",
      "The average call took 6.134ms\n",
      "Assuming linear scaling ‚Äî which is ALMOST NEVER true in practice, especially on GPU ‚Äî\n",
      "    \\_ this comes out to 95.8438 us/atom/call\n",
      "For this system, at a 1.00fs timestep, this comes out to 14.09 ns/day\n"
     ]
    }
   ],
   "source": [
    "!nequip-benchmark allegro/con!nequip-evaluate --train-dir results/silicon-tutorial/si --output output.xyzfigs/tutorial.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIQRZ1Yri8Du"
   },
   "source": [
    "This can be useful for understanding the effect of various hyperparameters on model performance on your particular system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXpcE3oP0LyD"
   },
   "source": [
    "## Evaluate test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wRKKCZ2PRl3"
   },
   "source": [
    "Before running a simulation, we'd like to know how well the model is doing on a hold-out test set. We run the `nequip-evaluate` command to compute the test error on all data that we didn't use for training or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mB54WSrN0PaS",
    "outputId": "264ee324-b57d-493a-aad6-4ee3fad7bdfc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n",
      "Loading model... \n",
      "loaded model from training session\n",
      "Loading original dataset...\n",
      "Loaded dataset specified in config.yaml.\n",
      "Using origial training dataset (110 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 50 frames.\n",
      "Starting...\n",
      "  0% 0/50 [00:00<?, ?it/s]\n",
      "\u001B[A\n",
      "  2% 1/50 [00:00<00:05,  8.27it/s]\n",
      "  4% 2/50 [00:00<00:12,  3.88it/s]\n",
      "  6% 3/50 [00:03<01:16,  1.63s/it]\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      " 24% 12/50 [00:03<00:09,  4.04it/s]\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      " 42% 21/50 [00:03<00:03,  8.50it/s]\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      " 60% 30/50 [00:04<00:01, 14.08it/s]\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      " 78% 39/50 [00:04<00:00, 20.82it/s]\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      "\u001B[A\n",
      " 96% 48/50 [00:04<00:00, 28.51it/s]\n",
      "\u001B[A\n",
      "100% 50/50 [00:04<00:00, 11.68it/s]\n",
      "\n",
      "\n",
      "--- Final result: ---\n",
      "               f_mae =  0.058606           \n",
      "              f_rmse =  0.075668           \n",
      "               e_mae =  0.084395           \n",
      "             e/N_mae =  0.001319           \n",
      "               f_mae =  0.058606           \n",
      "              f_rmse =  0.075668           \n",
      "               e_mae =  0.084395           \n",
      "             e/N_mae =  0.001319           \n"
     ]
    }
   ],
   "source": [
    "!nequip-evaluate --train-dir results/silicon-tutorial/si --batch-size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQHrMMnsPaJO"
   },
   "source": [
    "Again, we get an energy MAE of ~1meV/atom and a force MAE of <60 meV/A  üéâ\n",
    "\n",
    "`nequip-evaluate`'s `--output` option can also be used to generate an extXYZ file containing the predictions of the model for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJitSZgLYNNF"
   },
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo_kIpYV00as"
   },
   "source": [
    "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/deploy.png?raw=true\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VoeGtlA02KQ"
   },
   "source": [
    "We now convert the model to a potential file. This makes it independent of the Allegro code and we can use it any downstream application, such as LAMMPS. Note that you will now be able to call this standalone file without having the Allegro or `nequip` Python package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3NJJgtDIDNc",
    "outputId": "3a99fab8-b0b5-4949-b079-997a7f44520f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:root:Loading best_model from training session...\n",
      "INFO:root:Compiled & optimized model.\n",
      "si-deployed.pth\n"
     ]
    }
   ],
   "source": [
    "!nequip-deploy build --train-dir results/silicon-tutorial/si si-deployed.pth\n",
    "!ls *pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4r5FBXaum9n"
   },
   "source": [
    "## MD in LAMMPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qIYIYyr1B4O"
   },
   "source": [
    "We are now able to run MD with our potential. Here, we will run some short MD on a Si system and compute the Radial Distribution Function (RDF) $g(r)$ from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UirNBTlJ1BNZ"
   },
   "source": [
    "<img src=\"https://github.com/mir-group/nequip_mrs_tutorial/blob/master/run.png?raw=true\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQs0ijPhvAGb"
   },
   "source": [
    "### Set up a simple LAMMPS input file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ywwr4kmpVr9v"
   },
   "source": [
    "We will be running a simulation of Si at $T=300\\mathrm{K}$. We'll use a time step of 1fs, will run for 5,000 steps, and will compute the all-to-all RDF after some initial equilibration time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygAw9w4dVgY7"
   },
   "source": [
    "First, we'll write our initial frame to a LAMMPS data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3lQ9M_Jpj8l"
   },
   "outputs": [],
   "source": [
    "from ase.io import read, write\n",
    "\n",
    "example_atoms = read('./Si_data/sitraj.xyz', index=0)\n",
    "write('./si.data', example_atoms, format='lammps-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liVyQIAV4gr8"
   },
   "source": [
    "**IMPORTANT**:\n",
    "Note that Allegro and NequIP are agnostic to units. That means Allegro will learn energies and forces in whatever units your reference data are in. As a result, you will want to set the corresponding units in LAMMPS. Here, our reference data were in √Ö, eV, and eV/√Ö for positions, energies, and forces, respectively, so we'll be using `units metal`. This will obviously change if your reference data are in kcal/mol or Hartree for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tAHO8ODrpwG"
   },
   "outputs": [],
   "source": [
    "lammps_input = \"\"\"\n",
    "units\tmetal\n",
    "atom_style atomic\n",
    "dimension 3\n",
    "\n",
    "# set newton on for pair_allegro (off for pair_nequip)\n",
    "newton on\n",
    "boundary p p p\n",
    "read_data ../si.data\n",
    "\n",
    "# if you want to run a larger system, simply replicate the system in space\n",
    "# replicate 3 3 3\n",
    "\n",
    "# allegro pair style\n",
    "pair_style\tallegro\n",
    "pair_coeff\t* * ../si-deployed.pth Si\n",
    "\n",
    "mass 1 28.0855\n",
    "\n",
    "velocity all create 300.0 1234567 loop geom\n",
    "\n",
    "neighbor 1.0 bin\n",
    "neigh_modify delay 5 every 1\n",
    "\n",
    "timestep 0.001\n",
    "thermo 10\n",
    "\n",
    "# nose-hoover thermostat, 300K\n",
    "fix  1 all nvt temp 300 300 $(100*dt)\n",
    "\n",
    "# compute rdf and average after some equilibration\n",
    "comm_modify cutoff 7.0\n",
    "compute rdfall all rdf 1000 cutoff 5.0\n",
    "fix 2 all ave/time 1 2500 5000 c_rdfall[*] file si.rdf mode vector\n",
    "\n",
    "# run 5ps\n",
    "run 5000\n",
    "\"\"\"\n",
    "!rm -rf ./lammps_run\n",
    "!mkdir lammps_run\n",
    "with open(\"lammps_run/si_rdf.in\", \"w\") as f:\n",
    "    f.write(lammps_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDuyueY11YBF"
   },
   "source": [
    "### Run LAMMPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoM8NPB-AZat"
   },
   "source": [
    "We will now run LAMMPS. We've installed our plugin above and can now call the `pair_style allegro`. The output will print some general LAMMPS output (number of atoms, number of threads, ...), but it will also log the device Allegro is using and the type mapping. It's usually a good idea to check that this matches what you'd expect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIHjQCFQksGs"
   },
   "source": [
    "**CAUTION**:\n",
    "Plase note that if you want to run Allegro in parallel on multiple GPUs on your own cluster, make sure to run this with the `mpirun`command as explained [here](https://github.com/mir-group/pair_allegro). Here, since Colab only gives us one GPU, we will simply call the `lmp` binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gurLjNK5upvq",
    "outputId": "109e5a87-8f77-4146-a5ed-38bd284ec560"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LAMMPS (29 Sep 2021 - Update 2)\n",
      "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)\n",
      "  using 1 OpenMP thread(s) per MPI task\n",
      "Reading data file ...\n",
      "  orthogonal box = (0.0000000 0.0000000 0.0000000) to (10.862000 10.862000 10.862000)\n",
      "  1 by 1 by 1 MPI processor grid\n",
      "  reading atoms ...\n",
      "  64 atoms\n",
      "  read_data CPU = 0.001 seconds\n",
      "Allegro is using device cuda\n",
      "Allegro: Loading model from ../si-deployed.pth\n",
      "Allegro: Freezing TorchScript model...\n",
      "Type mapping:\n",
      "Allegro type | Allegro name | LAMMPS type | LAMMPS name\n",
      "0 | Si | 1 | Si\n",
      "Neighbor list info ...\n",
      "  update every 1 steps, delay 5 steps, check yes\n",
      "  max neighbors/atom: 2000, page size: 100000\n",
      "  master list distance cutoff = 6\n",
      "  ghost atom cutoff = 7\n",
      "  binsize = 3, bins = 4 4 4\n",
      "  2 neighbor lists, perpetual/occasional/extra = 1 1 0\n",
      "  (1) pair allegro, perpetual\n",
      "      attributes: full, newton on, ghost\n",
      "      pair build: full/bin/ghost\n",
      "      stencil: full/ghost/bin/3d\n",
      "      bin: standard\n",
      "  (2) compute rdf, occasional\n",
      "      attributes: half, newton on, cut 6\n",
      "      pair build: half/bin/atomonly/newton\n",
      "      stencil: half/bin/3d\n",
      "      bin: standard\n",
      "Setting up Verlet run ...\n",
      "  Unit style    : metal\n",
      "  Current step  : 0\n",
      "  Time step     : 0.001\n",
      "Per MPI rank memory allocation (min/avg/max) = 3.728 | 3.728 | 3.728 Mbytes\n",
      "Step Temp E_pair E_mol TotEng Press \n",
      "       0          300   -8324.1387            0   -8321.6957      2036.18 \n",
      "      10    189.18125   -8323.2334            0   -8321.6928    1284.0236 \n",
      "      20    79.011757   -8322.3281            0   -8321.6847    536.27386 \n",
      "      30    147.59319   -8322.8688            0   -8321.6669    1001.7544 \n",
      "      40    190.65855   -8323.1722            0   -8321.6196    1294.0504 \n",
      "      50    116.63762   -8322.5187            0   -8321.5689    791.65062 \n",
      "      60    99.941115   -8322.3461            0   -8321.5322    678.32699 \n",
      "      70     167.4203   -8322.8429            0   -8321.4796    1136.3262 \n",
      "      80    168.27992    -8322.786            0   -8321.4156    1142.1607 \n",
      "      90     144.2493    -8322.542            0   -8321.3673    979.05847 \n",
      "     100    195.92861   -8322.9156            0   -8321.3201    1329.8197 \n",
      "     110    194.15316   -8322.8469            0   -8321.2658    1317.7693 \n",
      "     120    120.75224    -8322.207            0   -8321.2237    819.57762 \n",
      "     130    153.42842   -8322.4358            0   -8321.1864    1041.3596 \n",
      "     140    247.26772    -8323.134            0   -8321.1205    1678.2719 \n",
      "     150    209.40522   -8322.7499            0   -8321.0446    1421.2891 \n",
      "     160    156.01796   -8322.2589            0   -8320.9884    1058.9355 \n",
      "     170    240.30278    -8322.881            0   -8320.9242     1630.999 \n",
      "     180    306.92319   -8323.3302            0   -8320.8308    2083.1695 \n",
      "     190    219.17824   -8322.5348            0     -8320.75    1487.6212 \n",
      "     200    153.62162   -8321.9479            0   -8320.6969    1042.6709 \n",
      "     210    202.46731   -8322.2878            0    -8320.639    1374.1996 \n",
      "     220     188.2763   -8322.0995            0   -8320.5663    1277.8814 \n",
      "     230    128.30127    -8321.554            0   -8320.5092    870.81494 \n",
      "     240    197.67479   -8322.0591            0   -8320.4493    1341.6715 \n",
      "     250    285.51792    -8322.678            0   -8320.3529    1937.8862 \n",
      "     260    235.48496   -8322.1762            0   -8320.2586    1598.2992 \n",
      "     270    187.77695   -8321.7205            0   -8320.1914    1274.4922 \n",
      "     280    286.21073   -8322.4458            0    -8320.115    1942.5886 \n",
      "     290    335.71102   -8322.7492            0   -8320.0154    2278.5602 \n",
      "     300    237.09917   -8321.8675            0   -8319.9367    1609.2553 \n",
      "     310    222.58585   -8321.6908            0   -8319.8782    1510.7495 \n",
      "     320    299.57311   -8322.2438            0   -8319.8042    2033.2826 \n",
      "     330    285.75518   -8322.0495            0   -8319.7224    1939.4966 \n",
      "     340    267.40118   -8321.8302            0   -8319.6526    1814.9231 \n",
      "     350    335.98151   -8322.3142            0   -8319.5782    2280.3961 \n",
      "     360    352.66493   -8322.3722            0   -8319.5003    2393.6309 \n",
      "     370    243.66131   -8321.4263            0    -8319.442    1653.7943 \n",
      "     380    200.39653   -8321.0307            0   -8319.3988    1360.1447 \n",
      "     390    278.57883   -8321.6104            0   -8319.3419    1890.7888 \n",
      "     400    235.99322   -8321.1945            0   -8319.2727    1601.7489 \n",
      "     410    170.42428   -8320.6075            0   -8319.2196     1156.715 \n",
      "     420    324.55103   -8321.7903            0   -8319.1473    2202.8144 \n",
      "     430    421.56453   -8322.4733            0   -8319.0403    2861.2708 \n",
      "     440    352.51486     -8321.83            0   -8318.9594    2392.6123 \n",
      "     450    303.88985   -8321.3826            0   -8318.9079    2062.5814 \n",
      "     460    386.49611   -8322.0078            0   -8318.8604    2623.2521 \n",
      "     470    390.40311   -8322.0004            0   -8318.8212      2649.77 \n",
      "     480    282.19864   -8321.0998            0   -8318.8018    1915.3574 \n",
      "     490     268.3806    -8320.972            0   -8318.7865    1821.5707 \n",
      "     500    362.67651   -8321.7196            0   -8318.7662    2461.5822 \n",
      "     510    327.92381   -8321.4199            0   -8318.7495    2225.7063 \n",
      "     520    283.68953   -8321.0507            0   -8318.7405    1925.4765 \n",
      "     530    340.85379   -8321.5067            0    -8318.731    2313.4656 \n",
      "     540    385.32484   -8321.8652            0   -8318.7273    2615.3024 \n",
      "     550    328.22846   -8321.4106            0   -8318.7377    2227.7741 \n",
      "     560    239.71744   -8320.7013            0   -8318.7492    1627.0262 \n",
      "     570    257.07852   -8320.8477            0   -8318.7542    1744.8605 \n",
      "     580    287.94759   -8321.0974            0   -8318.7525     1954.377 \n",
      "     590    337.10107   -8321.4941            0   -8318.7489    2287.9948 \n",
      "     600    411.38801   -8322.1043            0   -8318.7542    2792.2001 \n",
      "     610    365.61719    -8321.757            0   -8318.7796    2481.5413 \n",
      "     620    303.29687   -8321.2816            0   -8318.8117    2058.5567 \n",
      "     630    377.93367   -8321.9283            0   -8318.8507    2565.1366 \n",
      "     640    424.88601   -8322.3759            0   -8318.9159    2883.8146 \n",
      "     650    310.18009    -8321.522            0   -8318.9961     2105.275 \n",
      "     660    197.60485   -8320.6567            0   -8319.0475    1341.1968 \n",
      "     670    266.47485   -8321.2547            0   -8319.0847    1808.6358 \n",
      "     680    333.73171    -8321.847            0   -8319.1293    2265.1261 \n",
      "     690    277.54662    -8321.435            0   -8319.1749    1883.7829 \n",
      "     700    248.26063   -8321.2305            0   -8319.2088    1685.0111 \n",
      "     710    315.51072   -8321.8095            0   -8319.2402    2141.4554 \n",
      "     720    333.65372   -8321.9952            0   -8319.2782    2264.5968 \n",
      "     730    298.54157   -8321.7489            0   -8319.3178    2026.2812 \n",
      "     740    296.07995   -8321.7672            0   -8319.3561    2009.5736 \n",
      "     750    263.36978   -8321.5358            0   -8319.3911    1787.5609 \n",
      "     760    218.50021   -8321.1938            0   -8319.4145    1483.0192 \n",
      "     770    279.88573   -8321.7087            0   -8319.4295    1899.6591 \n",
      "     780    385.72372   -8322.5901            0    -8319.449    2618.0097 \n",
      "     790    363.85895   -8322.4485            0   -8319.4854    2469.6077 \n",
      "     800    261.45546   -8321.6523            0   -8319.5231    1774.5679 \n",
      "     810    281.80382   -8321.8482            0   -8319.5534    1912.6776 \n",
      "     820    312.52385   -8322.1309            0   -8319.5859    2121.1827 \n",
      "     830      245.333   -8321.6134            0   -8319.6156    1665.1404 \n",
      "     840    201.57336   -8321.2739            0   -8319.6324    1368.1321 \n",
      "     850      248.835   -8321.6665            0   -8319.6401    1688.9095 \n",
      "     860    252.31318   -8321.6951            0   -8319.6404    1712.5168 \n",
      "     870    223.94666   -8321.4568            0   -8319.6332    1519.9857 \n",
      "     880    316.13279   -8322.1919            0   -8319.6175    2145.6775 \n",
      "     890    372.09416   -8322.6275            0   -8319.5974    2525.5023 \n",
      "     900    282.40713   -8321.8864            0   -8319.5867    1916.7725 \n",
      "     910    229.23743    -8321.444            0   -8319.5772    1555.8956 \n",
      "     920    290.18305   -8321.9226            0   -8319.5595    1969.5497 \n",
      "     930    319.22181   -8322.1334            0   -8319.5338    2166.6436 \n",
      "     940    238.56417   -8321.4518            0   -8319.5091    1619.1986 \n",
      "     950     245.6643   -8321.4831            0   -8319.4826    1667.3891 \n",
      "     960    347.82509    -8322.272            0   -8319.4395    2360.7816 \n",
      "     970    344.37114   -8322.1957            0   -8319.3913    2337.3388 \n",
      "     980    298.55625   -8321.7851            0   -8319.3538    2026.3809 \n",
      "     990    293.66716   -8321.7109            0   -8319.3194    1993.1973 \n",
      "    1000    270.19748   -8321.4834            0   -8319.2831    1833.9023 \n",
      "    1010    223.92445   -8321.0695            0    -8319.246     1519.835 \n",
      "    1020    271.47509   -8321.4118            0    -8319.201    1842.5738 \n",
      "    1030    340.89842   -8321.9136            0   -8319.1375    2313.7685 \n",
      "    1040    274.24582   -8321.3098            0   -8319.0765    1861.3795 \n",
      "    1050     234.1763   -8320.9354            0   -8319.0284     1589.417 \n",
      "    1060    366.28872   -8321.9504            0   -8318.9676    2486.0992 \n",
      "    1070      447.246   -8322.5403            0   -8318.8982    3035.5778 \n",
      "    1080    345.13961   -8321.6717            0   -8318.8611    2342.5545 \n",
      "    1090    264.89344   -8321.0012            0   -8318.8441    1797.9024 \n",
      "    1100    328.64722   -8321.5035            0   -8318.8272    2230.6163 \n",
      "    1110    349.86528    -8321.661            0    -8318.812    2374.6289 \n",
      "    1120    255.04197   -8320.8804            0   -8318.8035    1731.0379 \n",
      "    1130    279.33233   -8321.0689            0   -8318.7942     1895.903 \n",
      "    1140    388.29563   -8321.9432            0   -8318.7812     2635.466 \n",
      "    1150    365.80562   -8321.7592            0   -8318.7803    2482.8203 \n",
      "    1160    291.28626   -8321.1618            0   -8318.7898    1977.0375 \n",
      "    1170    316.46086   -8321.3765            0   -8318.7994    2147.9043 \n",
      "    1180    360.02598   -8321.7459            0   -8318.8141    2443.5923 \n",
      "    1190    291.75547   -8321.2108            0   -8318.8349    1980.2221 \n",
      "    1200    233.31556   -8320.7491            0   -8318.8491    1583.5749 \n",
      "    1210    307.84209   -8321.3647            0   -8318.8578    2089.4064 \n",
      "    1220    352.47591   -8321.7386            0   -8318.8682     2392.348 \n",
      "    1230    349.22153   -8321.7313            0   -8318.8874    2370.2597 \n",
      "    1240    362.45814   -8321.8698            0   -8318.9182    2460.1001 \n",
      "    1250    343.70147    -8321.758            0   -8318.9591    2332.7935 \n",
      "    1260     327.2107   -8321.6716            0    -8319.007    2220.8663 \n",
      "    1270    309.68094   -8321.5779            0    -8319.056    2101.8871 \n",
      "    1280    317.02877   -8321.6893            0   -8319.1077    2151.7588 \n",
      "    1290    255.10963   -8321.2332            0   -8319.1558    1731.4971 \n",
      "    1300    190.28312    -8320.733            0   -8319.1835    1291.5022 \n",
      "    1310    303.93217   -8321.6785            0   -8319.2034    2062.8687 \n",
      "    1320    394.35444   -8322.4437            0   -8319.2323    2676.5887 \n",
      "    1330    337.64233   -8322.0241            0   -8319.2746    2291.6685 \n",
      "    1340    264.35925   -8321.4645            0   -8319.3117    1794.2767 \n",
      "    1350    274.71575   -8321.5758            0   -8319.3387     1864.569 \n",
      "    1360    303.70831   -8321.8354            0   -8319.3622    2061.3493 \n",
      "    1370    269.99298   -8321.5798            0   -8319.3811    1832.5144 \n",
      "    1380    260.65656   -8321.5148            0   -8319.3922    1769.1456 \n",
      "    1390    284.55336   -8321.7146            0   -8319.3974    1931.3395 \n",
      "    1400    279.62501   -8321.6754            0   -8319.3983    1897.8895 \n",
      "    1410    303.47055   -8321.8685            0   -8319.3973    2059.7355 \n",
      "    1420    343.47825   -8322.1945            0   -8319.3975    2331.2785 \n",
      "    1430    326.28815   -8322.0605            0   -8319.4034    2214.6046 \n",
      "    1440     282.3879   -8321.7103            0   -8319.4107     1916.642 \n",
      "    1450    279.77965   -8321.6929            0   -8319.4145    1898.9391 \n",
      "    1460    285.92599   -8321.7438            0   -8319.4154    1940.6559 \n",
      "    1470     241.6238   -8321.3802            0   -8319.4126    1639.9652 \n",
      "    1480    226.15625   -8321.2456            0   -8319.4039    1534.9828 \n",
      "    1490    299.47751    -8321.826            0   -8319.3873    2032.6337 \n",
      "    1500    334.55033   -8322.0911            0   -8319.3667    2270.6823 \n",
      "    1510    290.76903   -8321.7188            0   -8319.3509    1973.5269 \n",
      "    1520    321.92386   -8321.9594            0   -8319.3378    2184.9831 \n",
      "    1530    355.60805   -8322.2234            0   -8319.3275    2413.6066 \n",
      "    1540    268.93237   -8321.5139            0   -8319.3239    1825.3157 \n",
      "    1550    199.76213   -8320.9447            0    -8319.318    1355.8388 \n",
      "    1560    265.23245   -8321.4623            0   -8319.3024    1800.2034 \n",
      "    1570    353.12241   -8322.1503            0   -8319.2747    2396.7359 \n",
      "    1580    316.34624   -8321.8262            0   -8319.2501    2147.1263 \n",
      "    1590    255.97904   -8321.3163            0   -8319.2318     1737.398 \n",
      "    1600    299.58471   -8321.6494            0   -8319.2097    2033.3613 \n",
      "    1610    363.84557   -8322.1445            0   -8319.1816    2469.5169 \n",
      "    1620    377.16582   -8322.2328            0   -8319.1614     2559.925 \n",
      "    1630    289.06837   -8321.5079            0   -8319.1539    1961.9841 \n",
      "    1640    214.99663   -8320.8981            0   -8319.1473    1459.2394 \n",
      "    1650    245.08844   -8321.1282            0   -8319.1323    1663.4806 \n",
      "    1660    322.03351   -8321.7263            0   -8319.1039    2185.7273 \n",
      "    1670    351.49713   -8321.9328            0   -8319.0704    2385.7048 \n",
      "    1680    314.30286   -8321.6024            0   -8319.0429    2133.2573 \n",
      "    1690    322.78391   -8321.6485            0     -8319.02    2190.8205 \n",
      "    1700    356.30331   -8321.8996            0   -8318.9981    2418.3255 \n",
      "    1710    323.61501   -8321.6181            0   -8318.9828    2196.4613 \n",
      "    1720    261.22876   -8321.0997            0   -8318.9724    1773.0293 \n",
      "    1730    247.28983   -8320.9715            0   -8318.9577     1678.422 \n",
      "    1740    307.63014   -8321.4385            0   -8318.9333    2087.9678 \n",
      "    1750    344.88814   -8321.7088            0   -8318.9002    2340.8477 \n",
      "    1760    346.94444   -8321.6946            0   -8318.8693    2354.8044 \n",
      "    1770    336.36047   -8321.5832            0   -8318.8441    2282.9682 \n",
      "    1780    338.14864   -8321.5771            0   -8318.8234     2295.105 \n",
      "    1790     350.0824    -8321.658            0   -8318.8072    2376.1026 \n",
      "    1800    338.44237   -8321.5536            0   -8318.7975    2297.0986 \n",
      "    1810    333.23515   -8321.5086            0   -8318.7949    2261.7558 \n",
      "    1820    311.01757   -8321.3307            0    -8318.798    2110.9591 \n",
      "    1830    264.42622   -8320.9553            0   -8318.8019    1794.7313 \n",
      "    1840    271.53767   -8321.0135            0   -8318.8023    1842.9986 \n",
      "    1850    313.32478   -8321.3495            0    -8318.798    2126.6188 \n",
      "    1860    372.97583   -8321.8329            0   -8318.7956    2531.4864 \n",
      "    1870    386.79821   -8321.9572            0   -8318.8073    2625.3026 \n",
      "    1880     349.2959    -8321.682            0   -8318.8376    2370.7644 \n",
      "    1890    286.21383   -8321.2068            0    -8318.876    1942.6096 \n",
      "    1900    246.37205    -8320.915            0   -8318.9087    1672.1928 \n",
      "    1910     300.9126   -8321.3907            0   -8318.9403     2042.374 \n",
      "    1920    355.30509   -8321.8766            0   -8318.9832    2411.5504 \n",
      "    1930    311.09577   -8321.5707            0   -8319.0373      2111.49 \n",
      "    1940    263.50334    -8321.232            0   -8319.0862    1788.4674 \n",
      "    1950    321.55869   -8321.7536            0    -8319.135    2182.5046 \n",
      "    1960    389.87043    -8322.378            0   -8319.2031    2646.1545 \n",
      "    1970    347.31639    -8322.118            0   -8319.2897    2357.3289 \n",
      "    1980    260.48502   -8321.4853            0   -8319.3641    1767.9813 \n",
      "    1990    250.39771    -8321.458            0   -8319.4189     1699.516 \n",
      "    2000    272.59363   -8321.6871            0   -8319.4672    1850.1656 \n",
      "    2010    246.82147   -8321.5164            0   -8319.5064    1675.2431 \n",
      "    2020    230.66331   -8321.4108            0   -8319.5325    1565.5734 \n",
      "    2030     257.7145   -8321.6481            0   -8319.5494     1749.177 \n",
      "    2040    299.50634   -8322.0002            0   -8319.5612    2032.8294 \n",
      "    2050    352.50194   -8322.4462            0   -8319.5756    2392.5247 \n",
      "    2060    327.69137    -8322.266            0   -8319.5975    2224.1287 \n",
      "    2070    252.76756   -8321.6771            0   -8319.6187    1715.6008 \n",
      "    2080    210.77428   -8321.3457            0   -8319.6293    1430.5812 \n",
      "    2090    260.45819   -8321.7515            0   -8319.6305    1767.7992 \n",
      "    2100    314.39829   -8322.1862            0    -8319.626     2133.905 \n",
      "    2110    288.36361   -8321.9692            0   -8319.6209    1957.2007 \n",
      "    2120    290.91298   -8321.9849            0   -8319.6158     1974.504 \n",
      "    2130    299.39881   -8322.0471            0    -8319.609    2032.0996 \n",
      "    2140    267.07655   -8321.7759            0    -8319.601    1812.7197 \n",
      "    2150    256.43072   -8321.6774            0   -8319.5892    1740.4637 \n",
      "    2160    287.43498   -8321.9113            0   -8319.5706    1950.8978 \n",
      "    2170    302.67284   -8322.0112            0   -8319.5464    2054.3213 \n",
      "    2180    227.68466   -8321.3751            0   -8319.5209    1545.3565 \n",
      "    2190    197.61446    -8321.102            0   -8319.4928    1341.2621 \n",
      "    2200    287.30867   -8321.7851            0   -8319.4454    1950.0406 \n",
      "    2210     344.2254   -8322.1757            0   -8319.3726    2336.3496 \n",
      "    2220    332.07091   -8322.0024            0   -8319.2982    2253.8538 \n",
      "    2230    314.13066   -8321.7899            0   -8319.2318    2132.0885 \n",
      "    2240    314.75187   -8321.7324            0   -8319.1693    2136.3049 \n",
      "    2250    325.43044   -8321.7586            0   -8319.1085    2208.7831 \n",
      "    2260    322.60818   -8321.6792            0    -8319.052    2189.6277 \n",
      "    2270    286.95511   -8321.3406            0   -8319.0038    1947.6408 \n",
      "    2280    264.07838    -8321.112            0   -8318.9615    1792.3703 \n",
      "    2290    310.81939   -8321.4476            0   -8318.9165     2109.614 \n",
      "    2300    373.15453   -8321.9076            0   -8318.8689    2532.6993 \n",
      "    2310    353.38592   -8321.7121            0   -8318.8343    2398.5245 \n",
      "    2320    281.94413   -8321.1111            0   -8318.8151      1913.63 \n",
      "    2330    296.31411   -8321.2128            0   -8318.7998    2011.1629 \n",
      "    2340    339.26284   -8321.5474            0   -8318.7847    2302.6674 \n",
      "    2350     328.3287   -8321.4499            0   -8318.7762    2228.4544 \n",
      "    2360    293.60618   -8321.1645            0   -8318.7736    1992.7834 \n",
      "    2370    264.42541   -8320.9236            0   -8318.7703    1794.7257 \n",
      "    2380      304.515    -8321.244            0   -8318.7643    2066.8245 \n",
      "    2390    370.62545   -8321.7774            0   -8318.7592    2515.5338 \n",
      "    2400    424.16667   -8322.2227            0   -8318.7686    2878.9323 \n",
      "    2410    411.14725   -8322.1502            0    -8318.802     2790.566 \n",
      "    2420    323.12716   -8321.4757            0   -8318.8444    2193.1502 \n",
      "    2430    291.14264   -8321.2504            0   -8318.8795    1976.0627 \n",
      "    2440    279.44116   -8321.1823            0   -8318.9067    1896.6417 \n",
      "    2450    267.97264   -8321.1079            0   -8318.9257    1818.8018 \n",
      "    2460    293.66364   -8321.3306            0   -8318.9391    1993.1734 \n",
      "    2470    324.60718   -8321.5952            0   -8318.9518    2203.1955 \n",
      "    2480    310.92927   -8321.4986            0   -8318.9666    2110.3599 \n",
      "    2490    285.16391   -8321.3023            0   -8318.9801    1935.4835 \n",
      "    2500    323.95584   -8321.6306            0   -8318.9925    2198.7746 \n",
      "    2510    353.59639   -8321.8893            0   -8319.0098     2399.953 \n",
      "    2520    309.73843   -8321.5557            0   -8319.0334    2102.2773 \n",
      "    2530    270.03255   -8321.2532            0   -8319.0542    1832.7829 \n",
      "    2540    299.69168   -8321.5114            0   -8319.0709    2034.0874 \n",
      "    2550    346.01089    -8321.908            0   -8319.0903    2348.4682 \n",
      "    2560    329.83732   -8321.8028            0   -8319.1168    2238.6938 \n",
      "    2570    304.54095    -8321.626            0    -8319.146    2067.0006 \n",
      "    2580    306.73464   -8321.6728            0    -8319.175    2081.8898 \n",
      "    2590    343.22986   -8322.0029            0   -8319.2079    2329.5926 \n",
      "    2600    352.51682   -8322.1228            0   -8319.2521    2392.6257 \n",
      "    2610    258.76764   -8321.4036            0   -8319.2963    1756.3249 \n",
      "    2620    197.25407   -8320.9295            0   -8319.3232    1338.8159 \n",
      "    2630    240.40744   -8321.2965            0   -8319.3387    1631.7094 \n",
      "    2640    304.89074   -8321.8326            0   -8319.3497    2069.3747 \n",
      "    2650    325.46128   -8322.0133            0   -8319.3629    2208.9925 \n",
      "    2660    275.30233   -8321.6197            0   -8319.3778    1868.5503 \n",
      "    2670     253.7476   -8321.4543            0   -8319.3879    1722.2526 \n",
      "    2680     306.8188   -8321.8925            0    -8319.394     2082.461 \n",
      "    2690    339.81151   -8322.1691            0   -8319.4019    2306.3913 \n",
      "    2700     314.6962   -8321.9789            0   -8319.4162     2135.927 \n",
      "    2710    279.38915   -8321.7077            0   -8319.4325    1896.2887 \n",
      "    2720    282.03417   -8321.7441            0   -8319.4474    1914.2411 \n",
      "    2730    295.53559   -8321.8686            0   -8319.4619    2005.8788 \n",
      "    2740    281.69906   -8321.7713            0   -8319.4773    1911.9666 \n",
      "    2750    272.69586   -8321.7121            0   -8319.4914    1850.8595 \n",
      "    2760      312.295   -8322.0498            0   -8319.5067    2119.6294 \n",
      "    2770    326.55983   -8322.1893            0     -8319.53    2216.4486 \n",
      "    2780    249.51572   -8321.5898            0   -8319.5578    1693.5297 \n",
      "    2790    206.00887   -8321.2562            0   -8319.5786    1398.2371 \n",
      "    2800    244.01429   -8321.5816            0   -8319.5945      1656.19 \n",
      "    2810    247.43608   -8321.6206            0   -8319.6056    1679.4146 \n",
      "    2820    246.88326   -8321.6216            0   -8319.6112    1675.6625 \n",
      "    2830    293.98874   -8322.0055            0   -8319.6114    1995.3799 \n",
      "    2840    335.97141   -8322.3464            0   -8319.6104    2280.3275 \n",
      "    2850    326.30247   -8322.2718            0   -8319.6146    2214.7019 \n",
      "    2860    293.08586   -8322.0098            0   -8319.6231    1989.2519 \n",
      "    2870     280.2821   -8321.9139            0   -8319.6315    1902.3493 \n",
      "    2880    278.13409   -8321.9025            0   -8319.6376    1887.7702 \n",
      "    2890    273.75929   -8321.8698            0   -8319.6405    1858.0773 \n",
      "    2900    282.11038   -8321.9373            0   -8319.6399    1914.7583 \n",
      "    2910    258.10656   -8321.7381            0   -8319.6363     1751.838 \n",
      "    2920    200.74249   -8321.2609            0   -8319.6261    1362.4928 \n",
      "    2930    254.51162   -8321.6789            0   -8319.6063    1727.4382 \n",
      "    2940    351.48089   -8322.4317            0   -8319.5695    2385.5945 \n",
      "    2950    305.80478   -8322.0224            0   -8319.5321    2075.5786 \n",
      "    2960    213.40095   -8321.2411            0   -8319.5033    1448.4091 \n",
      "    2970    218.65687   -8321.2514            0   -8319.4708    1484.0825 \n",
      "    2980    304.31509   -8321.8971            0   -8319.4189    2065.4676 \n",
      "    2990    335.37947    -8322.083            0   -8319.3519    2276.3098 \n",
      "    3000    283.58145   -8321.6025            0   -8319.2931    1924.7429 \n",
      "    3010    302.50043   -8321.7038            0   -8319.2404    2053.1511 \n",
      "    3020    343.62182   -8321.9814            0   -8319.1832    2332.2529 \n",
      "    3030    356.20232   -8322.0321            0   -8319.1314    2417.6401 \n",
      "    3040    324.30338   -8321.7332            0   -8319.0922    2201.1335 \n",
      "    3050    299.95865   -8321.5056            0   -8319.0629    2035.8994 \n",
      "    3060    307.78069   -8321.5423            0    -8319.036    2088.9896 \n",
      "    3070    301.14544   -8321.4624            0     -8319.01    2043.9544 \n",
      "    3080    285.60879   -8321.3119            0   -8318.9861     1938.503 \n",
      "    3090    275.62886   -8321.2059            0   -8318.9614    1870.7666 \n",
      "    3100    264.16486   -8321.0854            0   -8318.9342    1792.9573 \n",
      "    3110    294.89813   -8321.3036            0   -8318.9021    2001.5522 \n",
      "    3120    344.15456   -8321.6674            0   -8318.8648    2335.8688 \n",
      "    3130    360.17757   -8321.7642            0   -8318.8311    2444.6212 \n",
      "    3140    344.24695   -8321.6119            0   -8318.8086    2336.4959 \n",
      "    3150    328.87325   -8321.4736            0   -8318.7954    2232.1504 \n",
      "    3160    310.85476   -8321.3192            0   -8318.7878    2109.8541 \n",
      "    3170    302.46823   -8321.2451            0    -8318.782    2052.9325 \n",
      "    3180    341.89385   -8321.5615            0   -8318.7774    2320.5247 \n",
      "    3190    398.23137   -8322.0241            0   -8318.7811    2702.9025 \n",
      "    3200    389.93038   -8321.9812            0   -8318.8058    2646.5615 \n",
      "    3210    304.22473    -8321.321            0   -8318.8436    2064.8543 \n",
      "    3220    267.33257   -8321.0554            0   -8318.8784    1814.4574 \n",
      "    3230    306.93346   -8321.4129            0   -8318.9134    2083.2392 \n",
      "    3240    326.46168   -8321.6157            0   -8318.9572    2215.7824 \n",
      "    3250    271.16481   -8321.2132            0    -8319.005    1840.4679 \n",
      "    3260    216.80187   -8320.8066            0   -8319.0411    1471.4921 \n",
      "    3270    281.71574   -8321.3672            0   -8319.0731    1912.0798 \n",
      "    3280    345.34869   -8321.9279            0   -8319.1156    2343.9737 \n",
      "    3290    333.90191   -8321.8917            0   -8319.1726    2266.2813 \n",
      "    3300    305.61727   -8321.7252            0   -8319.2364    2074.3059 \n",
      "    3310    312.47132   -8321.8501            0   -8319.3056    2120.8261 \n",
      "    3320     335.9403   -8322.1223            0   -8319.3866    2280.1163 \n",
      "    3330    314.83913   -8322.0414            0   -8319.4776    2136.8971 \n",
      "    3340    267.63315   -8321.7381            0   -8319.5587    1816.4975 \n",
      "    3350    254.44223   -8321.6954            0   -8319.6233    1726.9673 \n",
      "    3360    271.05966   -8321.8847            0   -8319.6773    1839.7542 \n",
      "    3370    283.59808   -8322.0332            0   -8319.7237    1924.8558 \n",
      "    3380    273.70669   -8321.9897            0   -8319.7608    1857.7203 \n",
      "    3390    251.16883   -8321.8317            0   -8319.7864    1704.7498 \n",
      "    3400    240.02584   -8321.7548            0   -8319.8002    1629.1194 \n",
      "    3410    246.27894   -8321.8103            0   -8319.8048    1671.5608 \n",
      "    3420    245.52475   -8321.8011            0   -8319.8017    1666.4419 \n",
      "    3430    253.46999   -8321.8568            0   -8319.7927    1720.3684 \n",
      "    3440    264.72912   -8321.9345            0   -8319.7787    1796.7871 \n",
      "    3450    262.19408   -8321.8965            0   -8319.7614    1779.5811 \n",
      "    3460    260.93574   -8321.8663            0   -8319.7414    1771.0404 \n",
      "    3470    290.70764   -8322.0854            0   -8319.7181    1973.1103 \n",
      "    3480    330.36593   -8322.3836            0   -8319.6933    2242.2816 \n",
      "    3490    325.51649   -8322.3243            0   -8319.6735    2209.3672 \n",
      "    3500    293.61259   -8322.0512            0   -8319.6602    1992.8269 \n",
      "    3510    251.64499   -8321.6975            0   -8319.6483    1707.9816 \n",
      "    3520    228.97873   -8321.4971            0   -8319.6325    1554.1397 \n",
      "    3530    273.44018   -8321.8358            0   -8319.6091    1855.9114 \n",
      "    3540    296.76355   -8321.9946            0   -8319.5779    2014.2133 \n",
      "    3550    248.33239   -8321.5695            0   -8319.5473    1685.4981 \n",
      "    3560    217.41438   -8321.2878            0   -8319.5173    1475.6494 \n",
      "    3570    275.96226   -8321.7252            0   -8319.4779    1873.0294 \n",
      "    3580    334.97648   -8322.1536            0   -8319.4258    2273.5746 \n",
      "    3590    277.91853   -8321.6416            0   -8319.3784    1886.3071 \n",
      "    3600    231.62527   -8321.2274            0   -8319.3412    1572.1024 \n",
      "    3610    306.30725   -8321.7922            0   -8319.2978     2078.989 \n",
      "    3620    346.04626   -8322.0645            0   -8319.2465    2348.7082 \n",
      "    3630    325.86554   -8321.8595            0   -8319.2059    2211.7363 \n",
      "    3640    319.64667   -8321.7786            0   -8319.1756    2169.5272 \n",
      "    3650    326.89447   -8321.8134            0   -8319.1514    2218.7199 \n",
      "    3660    334.55509   -8321.8577            0   -8319.1333    2270.7146 \n",
      "    3670    330.01196   -8321.8097            0   -8319.1223    2239.8792 \n",
      "    3680     310.4265    -8321.645            0    -8319.117    2106.9474 \n",
      "    3690     268.8671   -8321.3037            0   -8319.1142    1824.8727 \n",
      "    3700    233.02246   -8321.0049            0   -8319.1074    1581.5855 \n",
      "    3710    257.07593   -8321.1871            0   -8319.0937    1744.8429 \n",
      "    3720    316.19754   -8321.6474            0   -8319.0725     2146.117 \n",
      "    3730    318.91139   -8321.6478            0   -8319.0508    2164.5366 \n",
      "    3740    289.07619   -8321.3875            0   -8319.0334    1962.0372 \n",
      "    3750    311.92123   -8321.5571            0    -8319.017    2117.0926 \n",
      "    3760    334.39718   -8321.7245            0   -8319.0013    2269.6428 \n",
      "    3770    322.30577   -8321.6155            0   -8318.9908    2187.5752 \n",
      "    3780    333.76481   -8321.7036            0   -8318.9856    2265.3507 \n",
      "    3790    361.85647   -8321.9335            0   -8318.9868    2456.0163 \n",
      "    3800     328.3425    -8321.672            0   -8318.9982    2228.5481 \n",
      "    3810    269.47956   -8321.2059            0   -8319.0114    1829.0296 \n",
      "    3820    312.98704    -8321.571            0   -8319.0222    2124.3265 \n",
      "    3830    383.87079   -8322.1659            0   -8319.0399    2605.4334 \n",
      "    3840    325.46937   -8321.7222            0   -8319.0718    2209.0474 \n",
      "    3850     229.1897   -8320.9665            0   -8319.1001    1555.5716 \n",
      "    3860    258.38409   -8321.2236            0   -8319.1195    1753.7217 \n",
      "    3870    344.21573   -8321.9454            0   -8319.1424    2336.2839 \n",
      "    3880    294.69301   -8321.5737            0   -8319.1739      2000.16 \n",
      "    3890    200.65597   -8320.8327            0   -8319.1986    1361.9055 \n",
      "    3900    253.16047   -8321.2775            0   -8319.2159    1718.2676 \n",
      "    3910    352.72108   -8322.1102            0   -8319.2379     2394.012 \n",
      "    3920    363.20943   -8322.2372            0   -8319.2794    2465.1992 \n",
      "    3930    313.49982   -8321.8937            0   -8319.3407    2127.8069 \n",
      "    3940    286.36586   -8321.7445            0   -8319.4125    1943.6415 \n",
      "    3950    284.83387   -8321.8145            0   -8319.4949    1933.2434 \n",
      "    3960    291.13664   -8321.9579            0    -8319.587     1976.022 \n",
      "    3970    311.41564   -8322.2264            0   -8319.6904    2113.6609 \n",
      "    3980    276.14855   -8322.0423            0   -8319.7935    1874.2938 \n",
      "    3990    202.81308   -8321.5156            0    -8319.864    1376.5464 \n",
      "    4000    236.36014   -8321.8356            0   -8319.9108    1604.2393 \n",
      "    4010    302.10394   -8322.4165            0   -8319.9563      2050.46 \n",
      "    4020    256.74999   -8322.0862            0   -8319.9954    1742.6307 \n",
      "    4030    184.89302   -8321.5217            0    -8320.016    1254.9182 \n",
      "    4040    211.68184   -8321.7459            0   -8320.0221     1436.741 \n",
      "    4050    274.33538   -8322.2531            0   -8320.0191    1861.9873 \n",
      "    4060    250.25717   -8322.0479            0     -8320.01    1698.5621 \n",
      "    4070    219.27355   -8321.7821            0   -8319.9965     1488.268 \n",
      "    4080    281.06997   -8322.2635            0   -8319.9747    1907.6968 \n",
      "    4090    317.33784   -8322.5286            0   -8319.9444    2153.8565 \n",
      "    4100    276.29649   -8322.1666            0   -8319.9167    1875.2979 \n",
      "    4110    263.15857   -8322.0334            0   -8319.8904    1786.1274 \n",
      "    4120    288.37761   -8322.2078            0   -8319.8594    1957.2957 \n",
      "    4130    265.53767   -8321.9875            0   -8319.8251     1802.275 \n",
      "    4140    234.23663   -8321.6987            0   -8319.7913    1589.8264 \n",
      "    4150    252.14796   -8321.8051            0   -8319.7518    1711.3954 \n",
      "    4160    264.50141   -8321.8573            0   -8319.7033    1795.2416 \n",
      "    4170    254.61189   -8321.7254            0    -8319.652    1728.1188 \n",
      "    4180    273.21052   -8321.8232            0   -8319.5983    1854.3527 \n",
      "    4190    277.60732    -8321.802            0   -8319.5413    1884.1949 \n",
      "    4200    252.71009   -8321.5469            0    -8319.489    1715.2108 \n",
      "    4210    269.35392   -8321.6326            0   -8319.4391    1828.1769 \n",
      "    4220    327.23677     -8322.05            0   -8319.3851    2221.0432 \n",
      "    4230    316.93862   -8321.9178            0   -8319.3368    2151.1469 \n",
      "    4240    245.73677   -8321.3033            0   -8319.3022     1667.881 \n",
      "    4250    264.32463    -8321.424            0   -8319.2715    1794.0417 \n",
      "    4260    359.26493   -8322.1595            0   -8319.2339    2438.4269 \n",
      "    4270     367.3973   -8322.1953            0   -8319.2034    2493.6234 \n",
      "    4280    293.93187    -8321.582            0   -8319.1884    1994.9939 \n",
      "    4290    297.73997   -8321.6026            0    -8319.178    2020.8406 \n",
      "    4300    344.37517   -8321.9725            0   -8319.1682    2337.3661 \n",
      "    4310    310.43178   -8321.6919            0    -8319.164    2106.9833 \n",
      "    4320    244.07832   -8321.1492            0   -8319.1615    1656.6246 \n",
      "    4330    249.74623   -8321.1877            0   -8319.1539    1695.0942 \n",
      "    4340    291.18544   -8321.5101            0   -8319.1388    1976.3532 \n",
      "    4350    288.60548   -8321.4704            0   -8319.1202    1958.8424 \n",
      "    4360    289.61916   -8321.4592            0   -8319.1007    1965.7225 \n",
      "    4370    326.97188   -8321.7422            0   -8319.0796    2219.2453 \n",
      "    4380    331.65915   -8321.7612            0   -8319.0604     2251.059 \n",
      "    4390    310.20674   -8321.5728            0   -8319.0467    2105.4559 \n",
      "    4400    321.01251   -8321.6501            0    -8319.036    2178.7975 \n",
      "    4410    335.23381   -8321.7584            0   -8319.0285    2275.3213 \n",
      "    4420    310.65209   -8321.5554            0   -8319.0257    2108.4786 \n",
      "    4430    308.48046   -8321.5376            0   -8319.0255    2093.7391 \n",
      "    4440    326.22889   -8321.6842            0   -8319.0276    2214.2024 \n",
      "    4450    319.12979   -8321.6325            0   -8319.0337     2166.019 \n",
      "    4460    298.15592   -8321.4708            0   -8319.0428    2023.6637 \n",
      "    4470    328.78828   -8321.7321            0   -8319.0546    2231.5737 \n",
      "    4480    329.34579   -8321.7555            0   -8319.0735    2235.3577 \n",
      "    4490    248.88349   -8321.1212            0   -8319.0944    1689.2386 \n",
      "    4500    233.76269   -8321.0115            0   -8319.1079    1586.6097 \n",
      "    4510     328.8552   -8321.7975            0   -8319.1195    2232.0279 \n",
      "    4520    349.67156   -8321.9868            0   -8319.1393    2373.3141 \n",
      "    4530    264.88598    -8321.323            0   -8319.1659    1797.8518 \n",
      "    4540    258.29736   -8321.2921            0   -8319.1887     1753.133 \n",
      "    4550    340.64966   -8321.9904            0   -8319.2163    2312.0801 \n",
      "    4560    357.35691   -8322.1712            0   -8319.2612    2425.4766 \n",
      "    4570    294.43018   -8321.7153            0   -8319.3176    1998.3761 \n",
      "    4580    265.41512   -8321.5333            0   -8319.3719    1801.4432 \n",
      "    4590    291.14757   -8321.7998            0   -8319.4288    1976.0962 \n",
      "    4600    296.56416   -8321.9071            0   -8319.4921      2012.86 \n",
      "    4610    287.79977   -8321.9001            0   -8319.5565    1953.3737 \n",
      "    4620    274.52004   -8321.8543            0   -8319.6188    1863.2407 \n",
      "    4630    241.54303   -8321.6379            0   -8319.6709     1639.417 \n",
      "    4640    252.13083   -8321.7648            0   -8319.7116    1711.2792 \n",
      "    4650    307.47819   -8322.2557            0   -8319.7518    2086.9365 \n",
      "    4660     297.4415   -8322.2167            0   -8319.7946    2018.8148 \n",
      "    4670    219.43728   -8321.6137            0   -8319.8268    1489.3793 \n",
      "    4680    207.54872   -8321.5339            0   -8319.8437    1408.6885 \n",
      "    4690       265.24   -8322.0123            0   -8319.8524    1800.2546 \n",
      "    4700     277.9193   -8322.1189            0   -8319.8557    1886.3124 \n",
      "    4710    249.86899   -8321.8897            0   -8319.8549    1695.9275 \n",
      "    4720    261.90363   -8321.9809            0   -8319.8481    1777.6098 \n",
      "    4730    301.77925   -8322.2929            0   -8319.8354    2048.2563 \n",
      "    4740    287.68194   -8322.1633            0   -8319.8205     1952.574 \n",
      "    4750    271.00631   -8322.0115            0   -8319.8046    1839.3921 \n",
      "    4760    283.76266   -8322.0955            0   -8319.7847    1925.9728 \n",
      "    4770    258.57893   -8321.8661            0   -8319.7604    1755.0442 \n",
      "    4780    229.28851   -8321.5997            0   -8319.7325    1556.2422 \n",
      "    4790    248.63956   -8321.7201            0   -8319.6953     1687.583 \n",
      "    4800     288.5751   -8321.9933            0   -8319.6433    1958.6361 \n",
      "    4810    290.42083   -8321.9467            0   -8319.5817    1971.1636 \n",
      "    4820    271.33484   -8321.7294            0   -8319.5199    1841.6219 \n",
      "    4830    286.70295   -8321.7906            0   -8319.4558    1945.9294 \n",
      "    4840    298.37466   -8321.8174            0   -8319.3876    2025.1484 \n",
      "    4850      295.615   -8321.7302            0   -8319.3229    2006.4178 \n",
      "    4860    296.38077    -8321.678            0   -8319.2645    2011.6153 \n",
      "    4870    275.36369   -8321.4563            0   -8319.2139    1868.9668 \n",
      "    4880    268.02434   -8321.3535            0   -8319.1709    1819.1527 \n",
      "    4890    313.93905   -8321.6864            0   -8319.1298    2130.7881 \n",
      "    4900    359.84161   -8322.0235            0   -8319.0932    2442.3409 \n",
      "    4910    338.14884    -8321.825            0   -8319.0714    2295.1063 \n",
      "    4920    266.34688   -8321.2292            0   -8319.0603    1807.7673 \n",
      "    4930    289.82528   -8321.4101            0     -8319.05    1967.1215 \n",
      "    4940    365.58953   -8322.0165            0   -8319.0394    2481.3536 \n",
      "    4950    333.18091   -8321.7511            0   -8319.0378    2261.3877 \n",
      "    4960    261.32573   -8321.1691            0    -8319.041    1773.6874 \n",
      "    4970    268.00924   -8321.2224            0   -8319.0398    1819.0502 \n",
      "    4980    330.67587    -8321.728            0   -8319.0352    2244.3853 \n",
      "    4990    318.09533   -8321.6236            0   -8319.0332    2158.9978 \n",
      "    5000    291.04778   -8321.4042            0   -8319.0341    1975.4189 \n",
      "Loop time of 33.4233 on 1 procs for 5000 steps with 64 atoms\n",
      "\n",
      "Performance: 12.925 ns/day, 1.857 hours/ns, 149.596 timesteps/s\n",
      "98.4% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
      "\n",
      "MPI task timing breakdown:\n",
      "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
      "---------------------------------------------------------------\n",
      "Pair    | 33.205     | 33.205     | 33.205     |   0.0 | 99.35\n",
      "Neigh   | 0          | 0          | 0          |   0.0 |  0.00\n",
      "Comm    | 0.028878   | 0.028878   | 0.028878   |   0.0 |  0.09\n",
      "Output  | 0.020777   | 0.020777   | 0.020777   |   0.0 |  0.06\n",
      "Modify  | 0.15705    | 0.15705    | 0.15705    |   0.0 |  0.47\n",
      "Other   |            | 0.01107    |            |       |  0.03\n",
      "\n",
      "Nlocal:        64.0000 ave          64 max          64 min\n",
      "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
      "Nghost:        801.000 ave         801 max         801 min\n",
      "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
      "Neighs:        1387.00 ave        1387 max        1387 min\n",
      "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
      "FullNghs:      2944.00 ave        2944 max        2944 min\n",
      "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "Total # of neighbors = 2944\n",
      "Ave neighs/atom = 46.000000\n",
      "Neighbor list builds = 0\n",
      "Dangerous builds = 0\n",
      "Total wall time: 0:00:37\n",
      "[b18041c32546:03193] *** Process received signal ***\n",
      "[b18041c32546:03193] Signal: Segmentation fault (11)\n",
      "[b18041c32546:03193] Signal code: Address not mapped (1)\n",
      "[b18041c32546:03193] Failing at address: 0x7fdfcbf5420d\n",
      "[b18041c32546:03193] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fdfcf003980]\n",
      "[b18041c32546:03193] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fdfcec42775]\n",
      "[b18041c32546:03193] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fdfcf4ade44]\n",
      "[b18041c32546:03193] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fdfcec43605]\n",
      "[b18041c32546:03193] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fdfcf4abcb3]\n",
      "[b18041c32546:03193] *** End of error message ***\n"
     ]
    }
   ],
   "source": [
    "!cd lammps_run/ && ../lammps/build/lmp -in si_rdf.in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBqg-Khb9Ms8"
   },
   "source": [
    "### Plot results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IH-Ktxp9Egd"
   },
   "source": [
    "Finally, we'll plot the RDF. We see that we recover the Si-Si bond length of 0.235 nm quite nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "B9GGDRHgsVjs",
    "outputId": "d9e5993c-9d33-41b4-c1b1-8ba9ed70aded"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "rdf = parse_lammps_rdf('./lammps_run/si.rdf')  # utility function defined earlier\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(rdf[0][0], rdf[0][1], 'b', linewidth=5, label=\"Allegro, $T=300K$\")\n",
    "plt.xlabel('r [$\\AA$]')\n",
    "plt.ylabel('g(r)')\n",
    "plt.title(\"Si-Si bond length: {:.3f}$\\AA$\".format(rdf[0][0][np.argmax(rdf[0][1])]))\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i12diW-ebK-w"
   },
   "source": [
    "# Extending the NequIP framework\n",
    "\n",
    "The `nequip` framework, for which Allegro is an extension package, makes it easy to modify or extend models while preserving compatability with all the `nequip-*` tools and LAMMPS pair styles.\n",
    "\n",
    "To illustrate this, we demonstrate a simple modification to an Allegro model that adds random noise to the predicted pairwise energies before they are summed into per-atom and total energies. To do this, we define two kinds of `nequip` extensions:\n",
    "\n",
    " 1. A *module*, which includes code we want to put in our model. Modules are just PyTorch `torch.nn.Module`s that include some extra information for `nequip` about the irreps of the data they expect to input and output.\n",
    " 2. A *model builder*, a function that takes the config (and optionally a model returned by previous model builders) and returns a new version of the model. The model builder is responsible in this case for adding our new module to an existing Allegro model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aq1MBN3ocETo",
    "outputId": "78395482-3e67-4860-d0ed-b8db1edb5642"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing noise.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile noise.py\n",
    "import torch\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from nequip.data import AtomicDataDict\n",
    "from nequip.nn import GraphModuleMixin, SequentialGraphNetwork, AtomwiseReduce\n",
    "\n",
    "\n",
    "# First, we define a module that adds noise to a field:\n",
    "class AddNoiseModule(GraphModuleMixin, torch.nn.Module):\n",
    "    field: str\n",
    "    noise_sigma: float\n",
    "    _dim: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        field: str,\n",
    "        noise_sigma: float = 0.0,\n",
    "        irreps_in=None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.field = field\n",
    "        self.noise_sigma = noise_sigma\n",
    "        # We have to tell `GraphModuleMixin` what fields we expect in the input and output\n",
    "        # and what their irreps will be. Having basic geometry information (positions and edges)\n",
    "        # in the input is assumed.\n",
    "        # We will save the unmodified version of `field` in `field + '_noiseless'`\n",
    "        # we need to tell the framework what irreps this new output field\n",
    "        # `field + '_noiseless'` will have--- the same as `field`:\n",
    "        self._init_irreps(irreps_out={field + \"_noiseless\": irreps_in[field]}, irreps_in=irreps_in)\n",
    "        # this is just an e3nn.o3.Irreps...\n",
    "        field_irreps: o3.Irreps = self.irreps_in[field]\n",
    "        # ...whose properties we can save for later, for example:\n",
    "        self._dim = field_irreps.dim\n",
    "\n",
    "\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        \"\"\"Run the module.\n",
    "        The module both takes and returns an `AtomicDataDict.Type` = `Dict[str, torch.Tensor]`.\n",
    "        Keys that the module does not modify/add are expected to be propagated to the output unchanged.\n",
    "        \"\"\"\n",
    "        noiseless = data[self.field]\n",
    "        data[self.field + \"_noiseless\"] = noiseless\n",
    "        data[self.field] = noiseless + self.noise_sigma * torch.randn(\n",
    "            (len(noiseless), self._dim),\n",
    "            dtype=noiseless.dtype, device=noiseless.device\n",
    "        )\n",
    "        return data\n",
    "\n",
    "\n",
    "# Second, we define a model builder to add our new module to an Allegro model:\n",
    "def AddNoiseToPairEnergies(config, model: SequentialGraphNetwork) -> SequentialGraphNetwork:\n",
    "    model.insert_from_parameters(\n",
    "        # see allegro/models/_allegro.py for the names of all modules in an Allegro model\n",
    "        # `\"edge_eng\"` is the final readout MLP\n",
    "        after=\"edge_eng\",\n",
    "        # name for our new module\n",
    "        name=\"add_noise\",\n",
    "        # hardcoded parameters from the builder\n",
    "        params=dict(\n",
    "            field=\"edge_energy\"\n",
    "        ),\n",
    "        # config from which to pull other parameters--- this means we can set\n",
    "        # `noise_sigma` in our YAML config file!\n",
    "        shared_params=config,\n",
    "        # the module to add:\n",
    "        builder=AddNoiseModule,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UeB3Q4dhXgx"
   },
   "source": [
    "To use this extension, we specify `AddNoiseToPairEnergies` as a model builder after the `Allegro` model builder in our YAML config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsF-uTLjhxoy",
    "outputId": "67adb927-dc52-4122-ec8b-44221457a320"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# general\n",
      "root: results/silicon-tutorial\n",
      "run_name: si-noisy\n",
      "seed: 123456\n",
      "dataset_seed: 123456\n",
      "append: true\n",
      "default_dtype: float32\n",
      "\n",
      "noise_sigma: 0.01\n",
      "\n",
      "# -- network --\n",
      "model_builders:\n",
      " - allegro.model.Allegro\n",
      " - noise.AddNoiseToPairEnergies\n",
      " # the typical model builders from `nequip` can still be used:\n",
      " - PerSpeciesRescale\n",
      " - ForceOutput\n",
      " - RescaleEnergyEtc\n"
     ]
    }
   ],
   "source": [
    "# make a new config file with `noise.AddNoiseBuilder` added to `model_builders` after `allegro.model.Allegro`\n",
    "!perl -p -e 'print \" - noise.AddNoiseToPairEnergies\\n\" if $. == 12' allegro/configs/tutorial.yaml > allegro/configs/tutorial-extension.yaml\n",
    "# we can set options for our custom class using the YAML config:\n",
    "!perl -pi -e 'print \"noise_sigma: 0.01\\n\\n\" if $. == 9' allegro/configs/tutorial-extension.yaml\n",
    "# change the run name\n",
    "!sed -i -e \"s/run_name: si/run_name: si-noisy/\" allegro/configs/tutorial-extension.yaml\n",
    "# only train for 1 epoch since this is just an example\n",
    "!sed -i -e \"s/max_epochs: 100/max_epochs: 1/\" allegro/configs/tutorial-extension.yaml\n",
    "# print out the start of the updated YAML\n",
    "!head -n 18 allegro/configs/tutorial-extension.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do56YEA5h6Aw"
   },
   "source": [
    "We can now train our extended model just like a default Allegro or NequIP model:\n",
    "\n",
    "(We modify `PYTHONPATH` here so that the code in `noise.py` is importable; in real-world use you should create an importable Python package for your extension the standard way using `setup.py`, just like `allegro` does.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aOxZooFiCEK",
    "outputId": "f10c493e-0daf-48c3-af8c-305ceb3b0888"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33manony-moose-404286\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.13.3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/content/wandb/run-20220922_045358-thurles1\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33msi-noisy\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ‚≠êÔ∏è View project at \u001B[34m\u001B[4mhttps://wandb.ai/anony-moose-404286/allegro-tutorial?apiKey=3fdd1fcd9adf0ead3b8cf8d911319dcf43b512c9\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run at \u001B[34m\u001B[4mhttps://wandb.ai/anony-moose-404286/allegro-tutorial/runs/thurles1?apiKey=3fdd1fcd9adf0ead3b8cf8d911319dcf43b512c9\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
      "Torch device: cuda\n",
      "Successfully loaded the data set of type ASEDataset(110)...\n",
      "Replace string dataset_forces_rms to 0.9016265869140625\n",
      "Replace string dataset_per_atom_total_energy_mean to -129.9612274169922\n",
      "Atomic outputs are scaled by: [Si: 0.901627], shifted by [Si: -129.961227].\n",
      "Replace string dataset_forces_rms to 0.9016265869140625\n",
      "Initially outputs are globally scaled by: 0.9016265869140625, total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Number of weights: 37352\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      0     2         1.01        0.996       0.0136        0.718          0.9         6.71        0.105\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Initial Validation          0    0.499    0.002        0.992       0.0141         1.01        0.709        0.898         6.78        0.106\n",
      "Wall time: 0.49973956999997426\n",
      "! Best model        0    1.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      1    10        0.515        0.514      0.00132        0.513        0.646         2.09       0.0327\n",
      "      1    20        0.196        0.196     0.000197        0.315        0.399        0.811       0.0127\n",
      "      1    30        0.116        0.116     4.43e-05         0.25        0.308        0.384        0.006\n",
      "      1    40        0.104        0.104     3.17e-05        0.237         0.29        0.325      0.00508\n",
      "      1    50       0.0574       0.0573     6.52e-05        0.173        0.216        0.466      0.00728\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
      "      1     2       0.0639       0.0639      2.6e-05        0.181        0.228        0.241      0.00377\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
      "! Train               1   15.371    0.002        0.296     0.000895        0.297        0.354        0.491         1.08       0.0169\n",
      "! Validation          1   15.371    0.002       0.0731     3.44e-05       0.0731        0.186        0.244        0.257      0.00402\n",
      "Wall time: 15.371964803000083\n",
      "! Best model        1    0.073\n",
      "! Stop training: max epochs\n",
      "Wall time: 15.38767400200004\n",
      "Cumulative wall time: 15.38767400200004\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Waiting for W&B process to finish... \u001B[32m(success).\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                                                                                \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run history:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                 LR ‚ñÅ‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    cumulative_wall ‚ñÅ‚ñà\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:              epoch ‚ñÅ‚ñà\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   training_e/N_mae ‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:     training_e_mae ‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:     training_f_mae ‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    training_f_rmse ‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:      training_loss ‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    training_loss_e ‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    training_loss_f ‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: validation_e/N_mae ‚ñà‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   validation_e_mae ‚ñà‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   validation_f_mae ‚ñà‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  validation_f_rmse ‚ñà‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    validation_loss ‚ñà‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  validation_loss_e ‚ñà‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  validation_loss_f ‚ñà‚ñÅ\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:               wall ‚ñÅ‚ñà\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run summary:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                 LR 0.002\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    cumulative_wall 15.37073\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:              epoch 1\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   training_e/N_mae 0.01694\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:     training_e_mae 1.08426\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:     training_f_mae 0.35378\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    training_f_rmse 0.49079\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:      training_loss 0.29721\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    training_loss_e 0.0009\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    training_loss_f 0.29631\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: validation_e/N_mae 0.00402\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   validation_e_mae 0.25703\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   validation_f_mae 0.1856\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  validation_f_rmse 0.2438\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    validation_loss 0.07315\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  validation_loss_e 3e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  validation_loss_f 0.07311\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:               wall 15.37073\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Synced \u001B[33msi-noisy\u001B[0m: \u001B[34m\u001B[4mhttps://wandb.ai/anony-moose-404286/allegro-tutorial/runs/thurles1?apiKey=3fdd1fcd9adf0ead3b8cf8d911319dcf43b512c9\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Find logs at: \u001B[35m\u001B[1m./wandb/run-20220922_045358-thurles1/logs\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!rm -rf results/silicon-tutorial/si-noisy\n",
    "!PYTHONPATH=`pwd`:$PYTHONPATH nequip-train allegro/configs/tutorial-extension.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR_V9dtliCZZ"
   },
   "source": [
    "We can also compile and deploy it to a model that we could use in LAMMPS with `pair_allegro`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1FsSvm_iML4",
    "outputId": "01f61a0b-9d3e-4dd9-a083-19df27f4d503"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:root:Loading best_model from training session...\n",
      "INFO:root:Compiled & optimized model.\n",
      "si-deployed.pth  si-noisy-deployed.pth\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=`pwd`:$PYTHONPATH nequip-deploy build --train-dir results/silicon-tutorial/si-noisy si-noisy-deployed.pth\n",
    "!ls *pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4ZD6U0EkIp5"
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "This concludes our tutorial. A next step would be to head over to https://github.com/mir-group/nequip-allegro, install Allegro, and get started with your own system. If you have questions, please don't hesitate to reach out to `batzner[at]g[dot]harvard[dot]edu` and `albym[at]seas[dot]harvard[dot]edu`, we're happy to help! You can also ask questions on our [GitHub Discussions](https://github.com/mir-group/allegro/discussions).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
